var relearn_searchindex = [
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics",
    "content": "Often when dealing with problems in mathematics, we make use of certain facts. For example, when dealing with geometry problems on a flat surface, we often make use of the Pythagorean Theorem and the Angle Bisector Theorem. When dealing with quadratic equations, we can use the Quadratic Formula to calculate its roots. The Fundamental Theorem of Calculus gives us a way to relate integrals and derivatives.\nBut how do we know that the Pythagorean Theorem, Angle Bisector Theorem, or the Fundamental Theorem of Calculus are true? How do we know that the Quadratic Formula gives us the correct roots, and not some mere approximation, or worse outright lies about the roots? Can we trust that these theorems, or the myriad of other theorems, accurately describe reality?\nThe answer is that in order to establish these theorems as fact, they must be grounded in logic. If we use a system of logic to derive these theorems, then we can be confident in their application. While the study of logic is a deep philosophical rabbit hole, we can at least get by with the basics. Over the course of this chapter, we will become acquainted with the basic tools of logic. Even with the basic tools, a rich mathematical tapestry can be artfully weaved!",
    "description": "Often when dealing with problems in mathematics, we make use of certain facts. For example, when dealing with geometry problems on a flat surface, we often make use of the Pythagorean Theorem and the Angle Bisector Theorem. When dealing with quadratic equations, we can use the Quadratic Formula to calculate its roots. The Fundamental Theorem of Calculus gives us a way to relate integrals and derivatives.\nBut how do we know that the Pythagorean Theorem, Angle Bisector Theorem, or the Fundamental Theorem of Calculus are true? How do we know that the Quadratic Formula gives us the correct roots, and not some mere approximation, or worse outright lies about the roots? Can we trust that these theorems, or the myriad of other theorems, accurately describe reality?",
    "tags": [],
    "title": "Logic",
    "uri": "/the-book-of-foundational-mathematics/logic/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics",
    "content": "This is a new chapter.",
    "description": "This is a new chapter.",
    "tags": [],
    "title": "The Book of Foundational Mathematics",
    "uri": "/the-book-of-foundational-mathematics/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Probability",
    "content": "This is a new chapter.",
    "description": "This is a new chapter.",
    "tags": [],
    "title": "Foundations of Probability",
    "uri": "/the-book-of-probability/foundations-of-probability/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Logic",
    "content": "Logic provides a formal framework for expressing truth using precise symbolic systems. The most fundamental components of this framework are the statements we reason about - declarations that can be judged as being either true or false. On their own, these statements capture simple facts. Combining them in structured ways, we can express more complex ideas and uncover logical relationships.\nThis section explores the basic building blocks of logical expression and how they interact. By understanding these foundations, we lay the groundwork for more advanced reasoning - an essential tool in set theory, calculus, modern algebra, and combinatorics in addition to logic itself. In these areas, precise argumentation and structure play a role.\nPropositions The types of statements mentioned previously - declarative sentences that are either true or false, but never both - are the most fundamental component of mathematical logic.\nProposition A proposition is a declarative sentence; that is, a proposition is a sentence that is either true or false, but not both.\nSometimes the word statement is used.\nIt is very common to use lowercase letters as a shorthand for referencing various propositions, though other kinds of arbitrary symbols can be used as well.\nExample 1.1.1 Consider the following propositions:\n\\[ \\begin{align*} h \u0026: \\text{Hank helps manage a propane store.} \\\\ t \u0026: \\text{Mr. T was a mathematics major.} \\\\ p \u0026: \\text{Thomas Jefferson was the second president of the United States.} \\\\ x \u0026: 2 + 2 = 4\\ \\text{and}\\ 2 + 3 = 6. \\end{align*} \\]All of the sentences represented by $h, t, p$, and $x$ are either true or false. Here, propositions $h$ and $t$ are both true, while propositions $p$, and $x$ are both false. Notice that $x$ looks to be made up of simpler propositions:\n\\[ \\begin{align*} x_1 \u0026: 2 + 2 = 4. \\\\ x_2 \u0026: 2 + 3 = 6. \\end{align*} \\]Here, $x_1$ is a true proposition while $x_2$ is a false proposition.\nDetermining whether or not a proposition is true or false may take a variety of methods. Knowing that $h$ and $t$ are true is a matter of pop culture knowledge. Knowing that $p$ is a false proposition requires historical knowledge. Knowing the truth values of $x$, $x_1$, and $x_2$ requires mathematical knowledge.\nThe system of logic we’ll build up requires that we work with sentences that are either true or false, meaning there are a wide variety of sentences that are not propositions.\nExample 1.1.2 Consider the following sentences:\n\\[ \\begin{align*} \u0026 \\text{What time is it?} \\\\ \u0026 \\text{Are you hungry?} \\\\ \u0026 \\text{File your taxes before April 15.} \\\\ \u0026 \\text{What a beautiful painting!} \\end{align*} \\]These sentences can’t be described as true or false, so they are not propositions. There may be some confusion concerning whether or not we should regard yes and no as the same as true and false respectively. We sidestep this issue by avoiding dealing with questions. Instead, we could try and reword sentences to fit the description of a proposition, as in “You are hungry.”.\nCompound Propositions Looking back at Example 1.1.1, we see that proposition $x$ is made up of two propositions connected together by the word and. When a proposition is made up of simpler propositions joined together by connective words like and, or, if, and only if, the truth value of the proposition will depend on the truth values of its constituent parts.\nCompound Proposition, Primitive Proposition A compound proposition is a proposition made up of other propositions that are joined together with connectives such as and, or, and if then.\nA primitive proposition is a proposition that is not made up of other propositions, and thus can’t be broken up into smaller constituent propositions.\nCompound propositions are sometimes called non-primitive.\nThe following are the most commonly used logical connectives:\nand or if if … then only if if and only if necessary sufficient All of these connectives affect a compound proposition’s truth value in very different ways, as we’ll see shortly.\nExample 1.1.3 Consider the compound proposition\n$$\\text{Al sells women's shoes, and Peggy is a housewife.}$$This proposition is made up of the two primitive propositions\n\\[ \\begin{align*} \u0026 \\text{Al sells women's shoes.} \\\\ \u0026 \\text{Peggy is a housewife.} \\end{align*} \\]using the connective word and.\nLogical Connectives Compound propositions can be formed by joining together primitive propositions, other compound propositions, or a combination of both. Whether primitive or compound, we use connective words to join them together. We start by describing a simple transformation:\nNegation Let $p$ be any proposition (it could be primitive or compound).\nThe negation of $p$, denoted $\\neg p$, is the proposition “$\\text{Not}\\ p$”, or “$\\text{It is not the case that}\\ p$”.\nThe proposition $\\neg p$ is true whenever $p$ is false.\nNegations of primitive statements are not considered primitive.\nConjunction, Disjunction Let $p$ and $q$ be any two propositions (primitive or compound).\nThe conjunction of $p$ and $q$, denoted $p \\land q$ is the proposition “$p\\ \\text{and}\\ q$”. The proposition $p \\land q$ is true only when both $p$ and $q$ are true. $p \\land q$ is false otherwise.\nThe disjunction of propositions $p$ and $q$, denoted $p \\lor q$ is the proposition “$p\\ \\text{or}\\ q,\\ \\text{or both}$”. The proposition $p \\lor q$ is true when $p$ is true, $q$ is true, or both $p$ and $q$ are true.\nNotice that when we talk about the disjunction, we are using the word or in an inclusive sense. This is the commonly used form of the word or in math. Unless otherwise stated, assume or is being used in the inclusive sense.\nExclusive-or Let $p$ and $q$ be any two propositions (primitive or compound).\nThe exclusive-or of $p$ and $q$, denoted $p \\veebar q$, is the proposition “$p,\\ \\text{or}\\ q,\\ \\text{but not both}$”. The proposition $p \\veebar q$ is true when $p$ is true and $q$ is false, or $p$ is false and $q$ is true. $p \\veebar q$ is false when $p$ and $q$ are both false, or when $p$ and $q$ are both true.\nWith these four connectives, a wealth of propositions can be formed, especially when used in conjunction with each other.\n1.1.4 Consider the following propositions:\n\\[ \\begin{align*} p \u0026: \\text{Ricardo saves enough money to buy an AC/DC concert ticket.} \\\\ q \u0026: \\text{Ricardo's parents let him go to the concert.} \\end{align*} \\]We can translate the statement $\\neg (p \\lor q)$ as\n\\[ \\begin{align*} \u0026 \\text{It is not the case that Ricardo saved enough money to buy an} \\\\ \u0026 \\text{AC/DC concert ticket or his parents will let him go to the concert.} \\end{align*} \\]We can translate $\\neg (p \\land q)$ as\n\\[ \\begin{align*} \u0026 \\text{It is not the case that Ricardo saved enough money to buy an} \\\\ \u0026 \\text{AC/DC concert ticket and his parents will let him go to the concert.} \\end{align*} \\]We can translate $(\\neg p) \\lor q$ as\n\\[ \\begin{align*} \u0026 \\text{Ricardo did not enough money to buy an AC/DC concert} \\\\ \u0026 \\text{ticket or his parents will let him go to the concert.} \\end{align*} \\]We can translate $p \\land (\\neg q)$ as\n\\[ \\begin{align*} \u0026 \\text{Ricardo saved enough money to buy an AC/DC concert} \\\\ \u0026 \\text{ticket and his parents will not let him go to the concert.} \\end{align*} \\]We can translate $(\\neg p) \\veebar q$ as\n\\[ \\begin{align*} \u0026 \\text{Either Ricardo didn't save enough money to buy an AC/DC concert} \\\\ \u0026 \\text{ticket or his parents will let him go to the concert, but not both.} \\end{align*} \\] The last two logical connectives we’ll talk about are those that describe the way theorems are stated: conditional statements.\nImplication, Hypothesis, Conclusion Let $p$ and $q$ be any twp arbitrary propositions (primitive or compound).\nThe implication or conditional statement, denoted $p \\rightarrow q$, is the proposition “$\\text{If}\\ p, \\text{then}\\ q$”.\nThe proposition $p \\rightarrow q$ is false when p is true and q is false. It is true otherwise.\nHere, $p$ is called the hypothesis of the implication, and $q$ is called the conclusion of the implication.\nThere a quite a few more ways to translate an implication into English:\n$\\text{If}\\ p, \\text{then}\\ q$ $p\\ \\text{only if}\\ q$ $p\\ \\text{is a sufficient condition for}\\ q$ $p\\ \\text{is sufficient for }\\ q$ $q\\ \\text{is a necessary condition for}\\ p$ $q\\ \\text{is necessary for}\\ p$ $q\\ \\text{if}\\ p$ The necessary and sufficient parts can be confusing at first. It’s important to remember what information is being conveyed. When we say $p \\rightarrow q$, we mean that if we know $p$ occurred, then we automatically know $q$ occurred as well. This is why we say that $p$ is sufficient for $q$.\nAdditionally, when we say $p \\rightarrow q$, this means that if $q$ did not occur, then we also know that $p$ did not occur as well. This is why $p \\rightarrow q$ can be restated as $q\\ \\text{is necessary for}\\ p$. Knowing that $q$ occurred is necessary to knowing that $p$ occurred, but it is not enough, or sufficient to knowing that $p$ occurred.\nBiconditional Let $p$ and $q$ be any two arbitrary propositions (primitive or compound).\nThe biconditional of statements $p$ and $q$, denoted $p \\leftrightarrow q$ is the proposition “$p\\ \\text{if and only if}\\ q$”.\nThis proposition is true when $p$ and $q$ are both simultaneously true or both simultaneously false. This proposition is false when $p$ and $q$ have different truth values.\nFrom here, we are able to construct very elaborate and intricate compound propositions.\nExample 1.1.5 Consider the following propositions:\n\\[ \\begin{align*} a \u0026: \\text{Ricardo goes to the AC/DC concert.} \\\\ b \u0026: \\text{Ricardo finishes his homework.} \\\\ c \u0026: \\text{The concert is cancelled due to bad weather.} \\end{align*} \\]We can translate the proposition $(\\neg a \\lor b) \\rightarrow c$ as\n\\[ \\begin{align*} \u0026 \\text{If Ricardo does not go to the AC/DC concert or he finished his} \\\\ \u0026 \\text{homework then the concert is cancelled due to bad weather.} \\end{align*} \\]We can translate $a \\leftrightarrow (b \\land \\neg c)$ as\n\\[ \\begin{align*} \u0026 \\text{Ricardo goes to the AC/DC concert if and only if he finished his} \\\\ \u0026 \\text{homework and the concert is not cancelled due to bad weather.} \\end{align*} \\]We can translate $(a \\veebar b) \\rightarrow \\neg c$ as\n\\[ \\begin{align*} \u0026 \\text{If either Ricardo goes to the AC/DC concert or he doesn't finish his homework} \\\\ \u0026 \\text{(but not both), then the concert will not get cancelled due to bad weather.} \\end{align*} \\]We can translate $(\\neg a \\lor c) \\leftrightarrow b$ as\n\\[ \\begin{align*} \u0026 \\text{Ricardo doesn't go to the AC/DC concert, or the concert is cancelled} \\\\ \u0026 \\text{due to bad weather, if and only if Ricardo finished his homework.} \\end{align*} \\] A Closer Look at Implications Notice that in the definition of implication that the implication $p \\rightarrow q$ is false when $p$ is true and $q$ is false. In other words, we have that $\\text{true}\\ \\rightarrow\\ \\text{false}$ is a false proposition. This deserves special emphasis:\nTruth Value of an Implication $\\text{false}\\ \\rightarrow\\ \\text{false}$ is a true proposition.\n$\\text{false}\\ \\rightarrow\\ \\text{true}$ is a true proposition.\n$\\text{true}\\ \\rightarrow\\ \\text{false}$ is a false proposition.\n$\\text{true}\\ \\rightarrow\\ \\text{true}$ is a true proposition.\nThe reason why “$\\text{true}\\ \\rightarrow\\ \\text{false}$” is a false proposition is because we don’t want true statements leading to false statements in a logical system.\nCuriously enough, we do consider both “$\\text{false}\\ \\rightarrow\\ \\text{false}$” and “$\\text{false}\\ \\rightarrow\\ \\text{true}$” to be true propositions. This is because if we start with a false hypothesis, then the truth of the conclusion is irrelevant.\nTrivially True Implications of the form\n\\[ \\begin{align*} \u0026 \\text{false}\\ \\rightarrow\\ \\text{false} \\\\ \u0026 \\text{false}\\ \\rightarrow\\ \\text{true} \\end{align*} \\]are called trivially true.\nExample 1.1.6 Suppose Ricardo wants to buy two front row tickets to the AC/DC concert so he can take a friend. He decides the easiest way to buy the tickets is to save enough money by working a summer job. Two front row tickets cost $500.\nConsider the following propositions:\n\\[ \\begin{align*} s \u0026: \\text{Ricardo earns \\$500 by working a summer job.} \\\\ t \u0026: \\text{Ricardo buys two front row tickets to the AC/DC concert.} \\end{align*} \\]Let’s take a closer look at the implication $s \\rightarrow t$.\nCase 1: $\\text{false}\\ \\rightarrow\\ \\text{false}$ Here, Ricardo does not save $500 working a summer job and does not buy two front row tickets to the AC/DC concert. Because Ricardo was unable to save the needed money, Ricardo did not go back on his word. As far as we can tell, Ricardo would have bought the tickets if he had the money, it’s just that he wasn’t able to save the money, and was thus unable to follow through.\nThis is a trivially true implication.\nCase 2: $\\text{false}\\ \\rightarrow\\ \\text{true}$ Here, Ricardo wasn’t able to save $500, but still bought two front tow tickets to the AC/DC concert. Perhaps he won some money in a contest, or was gifted money by friends or family. In this case, Ricardo did not go back on his word of saving money to buy tickets. Again, he may have bought the tickets if he did save money working a summer job.\nThis is a trivially true implication.\nCase 3: $\\text{true}\\ \\rightarrow\\ \\text{false}$ In this case, Ricardo did save $500 working a summer job, but failed to buy the tickets. Here, Ricardo did go back on his word. This means that the proposition \\(s \\rightarrow t\\) is not an accurate description of reality. Ricardo fulfilled the premise but did not follow through with the conclusion.\nThe implication is a false one.\nCase 4: $\\text{true}\\ \\rightarrow\\ \\text{true}$ In this case, Ricardo saved $500 working a summer job, and bought two front row tickets to the AC/DC concert. Ricardo kept his word, and fulfilled the resolution.\nThis is a true implication, but not a trivially true implication.",
    "description": "Logic provides a formal framework for expressing truth using precise symbolic systems. The most fundamental components of this framework are the statements we reason about - declarations that can be judged as being either true or false. On their own, these statements capture simple facts. Combining them in structured ways, we can express more complex ideas and uncover logical relationships.\nThis section explores the basic building blocks of logical expression and how they interact. By understanding these foundations, we lay the groundwork for more advanced reasoning - an essential tool in set theory, calculus, modern algebra, and combinatorics in addition to logic itself. In these areas, precise argumentation and structure play a role.",
    "tags": [],
    "title": "Propositions",
    "uri": "/the-book-of-foundational-mathematics/logic/propositions/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Functions",
    "content": "In this chapter’s introduction, we mentioned that a function is a specific kind of relation, which means there are additional requirements for a relation to be a function that extend beyond simply being a subset of some cross product $A \\times B$. Before we give the definition, let’s look at some relations that behave differently from one another to get a more intuitive sense of what we’re looking for in a function.\nThree Examples Example 6.1.1 Consider the relation $\\mathcal{R}_1 \\subseteq \\{1, 2, 3, 4\\} \\times \\{a, b, c, d\\}$ where $$\\mathcal{R}_1 = \\{(2, d), (3, b), (4, b)\\}$$ Figure 6.1.1\nNotice that elements $2, 3, 4$ are related to elements from $\\{a, b, c, d\\}$, but what about the element $1$? Even though $\\mathcal{R}_1$ is a legitimate relation, we may want to have a way to relate all elements in $\\{1, 2, 3, 4\\}$. Example 6.1.2 Consider the relation $\\mathcal{R}_2 \\subseteq \\{1, 2, 3, 4\\} \\times \\{a, b, c, d\\}$ where $$\\mathcal{R}_2 = \\{(1, d), (2, c), (2, a), (3, b), (4, a)\\}$$ Figure 6.1.2\nHere, we have related every element in $\\{1, 2, 3, 4\\}$ to at least one element in $\\{a, b, c, d\\}$. However, the element $2$ is related to two different elements: $a, c$. Again, this is a valid relation, but perhaps we want to limit how many elements can be related to some given element. $$~$$ For example, suppose we needed to pick an element related to $2$. How would we choose between $a$ and $c$? It's ambiguous unless we have some heuristic to settle the matter. However, if $2$ was only related to one element, then the choice is automatic and not ambiguous. Example 6.1.3 Consider the relation $\\mathcal{R}_3 \\subseteq \\{1, 2, 3, 4\\} \\times \\{a, b, c, d\\}$ where $$\\mathcal{R}_3 = \\{(1, a), (2, c), (3, d), (4, c)\\}$$ Figure 6.1.3\nHere, every element in $\\{1, 2, 3, 4\\}$ is related to some element in $\\{a, b, c, d\\}$, and each element in $\\{1, 2, 3, 4\\}$ is related to exactly one element in $\\{a, b, c, d\\}$. This avoids the problems that $\\mathcal{R}_1$ and $\\mathcal{R}_2$ have in Example 6.1.1 and Example 6.1.2 above. Defining What a Function Is We define functions in a way to avoid all of the shortcomings present in the relations defined in Example 6.1.1 and Example 6.1.2. Before we give the definition though, we introduce one more quantifier.\nRemember that quantifiers are used in conjunction with propositions that have variables. In addition to the Universal Quantifier and the Existential Quantifier, we also have the Uniqueness Quantifier, which we denote using the notation $\\exists!$, which is just the Existential Quantifier with an exclamation mark tacked on. The Uniqueness Quantifier is used to specify that a given proposition is true for exactly one element $x$ from the underlying universe $\\mathcal{U}$. We write this as\n$$\\exists! x \\in \\mathcal{U}\\ [p(x)]$$ Example 6.1.4 Reconsider the relation $\\mathcal{R}_3$ from Example 6.1.3. We have that\n$$\\exists!\\ x \\in \\{1, 2, 3, 4\\}\\ [x\\ \\mathcal{R}_3\\ a] = 1$$because there is a value of $x$ (namely $1$) such that $(x, a) \\in \\mathcal{R}_3$, and that value is the only value of $x$ such that $(x, a) \\in \\mathcal{R}_3$. This is because $2\\ \\mathcal{R}_3\\ c, 3\\ \\mathcal{R}_3\\ d, 4\\ \\mathcal{R}_3\\ c$.\nSimilarly, we have that\n$$\\exists!\\ x \\in \\{1, 2, 3, 4\\}\\ [x\\ \\mathcal{R}_3\\ d] = 1$$because only one element in $\\{1, 2, 3, 4\\}$ is related to $d$.\nHowever, note that\n$$\\exists!\\ x \\in \\{1, 2, 3, 4\\}\\ [x\\ \\mathcal{R}_3\\ c] = 0$$because there is more than one element in {1, 2, 3, 4} that is related to $c$. Those values are $2, 4$.\nFurthermore, we also have that\n$$\\exists!\\ x \\in \\{1, 2, 3, 4\\}\\ [x\\ \\mathcal{R}_3\\ b] = 0$$because no element is related to $b$.\nNow we formally state the definition of function:\nFunction, Mapping Let $A$ and $B$ be arbitrary sets, and let $f$ be an arbitrary relation defined on $A \\times B$. $f$ is called a function (also sometimes a mapping) from $A$ to $B$, and we write\n$$f: A \\rightarrow B$$if (and only if) every element in $A$ is related to exactly one element in $B$ under $f$. Logically, we write\n$$\\forall a \\in A\\ \\exists! b \\in B\\ [(a, b) \\in f]$$ Notice that the definition applies with arbitrary sets. Both $A, B$ could be the Empty set, in which case we’d have $f: \\emptyset \\rightarrow \\emptyset$, which is commonly called the empty function. If only one of $A, B$ were the empty set, then the only function possible would be the empty function (as a matter of fact, the only relation that could be defined would be the empty relation).\nExample 6.1.5 Reconsider the sets $A = \\{1, 2, 3, 4\\}$ and $B = \\{a, b, c, d\\}$ from the previous examples.\na.) $\\mathcal{R}_1 = \\{(2, d), (3, b), (4, b)\\}$ The definition of function says that every element in $\\{1, 2, 3, 4\\}$ must be related to exactly one element in $\\{a, b, c, d\\}$. We need to examine the proposition $\\exists!y \\in B\\ [(x, y) \\in \\mathcal{R}_1]$ for each $x \\in A$. We’ll tabulate our results in the following list:\n$\\exists!y \\in B\\ [(1, y) \\in \\mathcal{R}_1] = 0\\ $ ($1$ is not related to any element in $\\{a, b, c, d\\}$) $\\exists!y \\in B\\ [(2, y) \\in \\mathcal{R}_1] = 1$ $\\exists!y \\in B\\ [(3, y) \\in \\mathcal{R}_1] = 1$ $\\exists!y \\in B\\ [(4, y) \\in \\mathcal{R}_1] = 1$ One of the above propositions evaluated to 0, meaning not all elements in $\\{1, 2, 3, 4\\}$ are related to exactly one element from $\\{a, b, c, d\\}$. Hence, $\\mathcal{R}_1$ is not a function.\nb.) $\\mathcal{R}_2 = \\{(1, d), (2, c), (2, a), (3, b), (4, a)\\}$ We can tabulate the propositions to check in another list:\n$\\exists!y \\in B\\ [(1, y) \\in \\mathcal{R}_2] = 1$ $\\exists!y \\in B\\ [(2, y) \\in \\mathcal{R}_2] = 0\\ $ ($2$ is related to more than one element in $\\{a, b, c, d\\}$) $\\exists!y \\in B\\ [(3, y) \\in \\mathcal{R}_2] = 1$ $\\exists!y \\in B\\ [(4, y) \\in \\mathcal{R}_2] = 1$ Again, one of the above propositions evaluated to false, so $\\mathcal{R}_2$ is not a function.\nc.) $\\mathcal{R}_3 = \\{(1, a), (2, c), (3, d), (4, c)\\}$ Let’s check all the necessary propositions:\n$\\exists!y \\in B\\ [(1, y) \\in \\mathcal{R}_3] = 1$ $\\exists!y \\in B\\ [(2, y) \\in \\mathcal{R}_3] = 1$ $\\exists!y \\in B\\ [(3, y) \\in \\mathcal{R}_3] = 1$ $\\exists!y \\in B\\ [(4, y) \\in \\mathcal{R}_3] = 1$ Since the proposition $\\exists!y \\in B\\ [(x, y) \\in \\mathcal{R}_3]$ was true for all $x \\in \\{1, 2, 3, 4\\}$, we have that\n$$\\forall x \\in A\\ \\exists! y \\in B\\ [(x, y) \\in \\mathcal{R}_3] = 1$$meaning $\\mathcal{R}_3$ is a function.\nBecause functions are special types of relations, all of the normal notations we used for relations also apply to functions. For example, for some arbitrary function $f: A \\rightarrow B$, if $a \\in A$ is related to $b \\in B$ under $f$, we can write $(a, b) \\in f$, and $a\\ f\\ b$.\nWhen it comes to functions specifically, we introduce a new notation to signal that element $a \\in A$ is related to element $b \\in B$:\n$$f(a) = b$$This notation is very common, and will be adopted in this book henceforth.\nWe also introduce new terminology to refer to $a$ and $b$ which will come in handy from time to time:\nImage, Preimage Let $f: A \\rightarrow B$ be an arbitrary function with $a \\in A$ and $b \\in B$ where $f(a) = b$.\nThe element $b$ is called the image of element $a$ under $f$. The element $a$ is the preimage of element $b$ under $f$.\nWe also have a few terms to describe the sets over which functions are defined.\nDomain, Codomain, Range Let $f: A \\rightarrow B$ be an arbitrary function.\nThe set $A$ is referred to as the domain of $f$ and is often written as $\\text{Dom}(f)$.\nThe set $B$ is referred to as the codomain of $f$ and is often written as $\\text{Cod}(f)$.\nThe subset of $B$ consisting of all the images from the elements of $A$ under $f$ is called the range of $f$ and is written as $\\text{Rng}(f)$.\nExample 6.1.6 Consider the function $f: \\{1, 2, 3, 4\\} \\rightarrow \\{a, b, c, d\\}$ where\n\\[ \\begin{align*} f(1) \u0026= c \\\\ f(2) \u0026= b \\\\ f(3) \u0026= d \\\\ f(4) \u0026= a \\end{align*} \\]The image of $1$ under $f$ is the element $c$. The preimage of element $a$ under $f$ is the element $4$.\nWe can also see that\n\\[ \\begin{align*} \\text{Dom}(f) \u0026= \\{1, 2, 3, 4\\} \\\\ \\text{Cod}(f) \u0026= \\{a, b, c, d\\} \\\\ \\end{align*} \\]Finally, we observe that every element in $B$ is an image under $f$, meaning we have\n$$\\text{Rng}(f) = \\{a, b, c, d\\} = B$$ Example 6.1.7 Reconsider the relation $\\mathcal{R}_3 \\subseteq \\{1, 2, 3, 4\\} \\times \\{a, b, c, d\\}$ from Example 6.1.3 where\n$$\\mathcal{R}_3 = \\{(1, a), (2, c), (3, d), (4, c)\\}$$Since $\\mathcal{R}_3$ is a function, we’ll use a lowercase letter to refer to it. This time, we’ll use the letter $g$ just for the sake of variety:\n\\[ \\begin{align*} g(1) \u0026= a \\\\ g(2) \u0026= c \\\\ g(3) \u0026= d \\\\ g(4) \u0026= c \\end{align*} \\]Here, we see that $d$ is the image of $3$ under $g$, and that $1$ is the preimage of $a$ under $g$.\nWe see that we have the same domain and codomain as function $f$ from Example 6.1.6:\n\\[ \\begin{align*} \\text{Dom}(g) \u0026= \\{1, 2, 3, 4\\} \\\\ \\text{Cod}(g) \u0026= \\{a, b, c, d\\} \\end{align*} \\]This time however, the element $b$ is not the image of some element in $\\{1, 2, 3, 4\\}$ under $g$. Thus we have that\n$$\\text{Rng}(g) = \\{a, c, d\\} \\subset \\{a, b, c, d\\} = B$$ If we were only interested in the images of some particular subset of a function’s domain, we could collect them in their own set.\nImage Set Let $f: A \\rightarrow B$ be an arbitrary function, and let $A_1 \\subseteq A$.\nThe set\n$$f(A_1) = \\{f(a)\\ |\\ a \\in A_1\\}$$is called the image set of $A_1$ under $f$.\nBased on the definitions given, we see that the range of a function is equal to the image set of that function’s domain; in other words,\n$$\\text{Rng}(f) = f(\\text{Dom}(f))$$ Example 6.1.8 Consider the function $f: \\{1, 2, 3, 4\\} \\rightarrow \\{a, b, c, d\\}$ where\n\\[ \\begin{align*} f(1) \u0026= b \\\\ f(2) \u0026= d \\\\ f(3) \u0026= a \\\\ f(4) \u0026= c \\end{align*} \\]The image set of {1, 3} would be\n$$f(\\{1, 3\\}) = \\{b, a\\}$$ Subsets and Supersets of a Function’s Domain Notice that a function is defined based in part on it’s domain. This means we can easily make new functions by simply taking subsets and supersets of the original function’s domain.\nRestriction Let $f: A \\rightarrow B$ be an arbitrary function, and let $A_1 \\subseteq A$.\nThe function $f|_{A_1}: A_1 \\rightarrow B$ is called the restriction of $f$ to $A_1$ if (and only if)\n$$\\forall a \\in A_1\\ [f|_{A_1}(a) = f(a)]$$ Extension Let $f: A_1 \\rightarrow B$ be an arbitrary function, and let $A_1 \\subseteq A$.\nThe function $g: A \\rightarrow B$ is called the extension of $f$ to $A$ if (and only if)\n$$\\forall a \\in A_1\\ [g(a) = f(a)]$$ Essentially, a restriction of a function is simply a new function defined on a subset of the original function’s domain while keeping the original function intact within the new function. Extensions are simply new functions defined on a superset of the original function’s domain.\nExample 6.1.9 Consider the function $f: \\{1, 2, 3, 4\\} \\rightarrow \\{a, b, c, d\\}$ where\n\\[ \\begin{align*} f(1) \u0026= b \\\\ f(2) \u0026= a \\\\ f(3) \u0026= d \\\\ f(4) \u0026= c \\end{align*} \\]One restriction we could define from $f$ is on the subset $\\{1, 3, 4\\}$, which we’ll call $g$:\n\\[ \\begin{align*} g(1) \u0026= b \\\\ g(3) \u0026= d \\\\ g(4) \u0026= c \\end{align*} \\]One extension we could define on $f$, which we’ll call $h$, will have domain $\\{1, 2, 3, 4, 5\\}$:\n\\[ \\begin{align*} h(1) \u0026= b \\\\ h(2) \u0026= a \\\\ h(3) \u0026= d \\\\ h(4) \u0026= c \\\\ h(5) \u0026= d \\end{align*} \\]",
    "description": "In this chapter’s introduction, we mentioned that a function is a specific kind of relation, which means there are additional requirements for a relation to be a function that extend beyond simply being a subset of some cross product $A \\times B$. Before we give the definition, let’s look at some relations that behave differently from one another to get a more intuitive sense of what we’re looking for in a function.",
    "tags": [],
    "title": "Functions",
    "uri": "/the-book-of-foundational-mathematics/functions/functions/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Relations",
    "content": "Just as we saw how sets can be built via propositional logic, we’ll see how relations and functions are built up using sets, a clear example of how multiple seemingly disparate concepts can be accumulated into a cohesive whole. But before we delve into what relations and functions are exactly, there is one more set operation we should get acquainted with: the set product. Ironically, even though order doesn’t matter when considering elements in a set, constructing a set product does require that we know in what order we are doing things. In this section, we discuss a new type of mathematical object that takes order into account.\nOrdered Pairs When we were discussing sets, we noted that order and repetition did not matter. All that mattered was whether or not an element was included in the set or not. Sometimes, we do care about the order in which elements are listed. In such a case, we list the elements between parentheses ( ) instead of curly braces { }.\nOrdered Pair An ordered pair is an ordered collection of two elements a and b, and is normally written using parentheses as in\n$$\\left(a, b\\right)$$where element a is the first item in the pair, and element b is the second item.\nNotice that the definition of ordered pair does not require that elements a and b have to come from the same universe (set). We can insert anything we want in an ordered pair.\nContrast ordered pairs with sets. Because order is irrelevant in sets, we have that\n$$ \\{a, b\\} = \\{b, a\\} $$whereas for ordered pairs, we have that (a, b) and (b, a) are different. In order for two different ordered pairs to be the same, we would need the corresponding parts to be the same.\nEqual Two ordered pairs (a, b) and (c, d) are called equal and we write\n$$\\left(a, b\\right) = \\left(c, d\\right)$$if (and only if) a = c and b = d.\nExample 5.1.1 The following are all examples of ordered pairs:\n\\[ \\begin{array}{ l l l } (1, -1) \u0026 (0, 12) \u0026 (12, 2.0013) \\\\ (5, \\text{tokyo}) \u0026 (3, 4) \u0026 (\\text{red}, -13) \\\\ (-\\pi^e, \\sqrt{2}) \u0026 (0, 12) \u0026 (0, -9) \\end{array} \\]Let’s let (a, b) refer to the upper-middle ordered pair above (0, 12), and let’s refer to the lower-middle ordered pair above (0, 12) as (c, d). Doing this tells us that\n\\[ \\begin{array}{ l l } a = 0 \u0026 b = 12 \\\\ c = 0 \u0026 d = 12 \\end{array} \\]Of course, we can easily see that since\n$$ a = c = 0 $$$$ b = d = 12 $$we must then have that (a, b) = (c, d), meaning that we really have that\n$$ (0, 12) = (0, 12) $$as expected. This is true by definition, but this is also easily seen because the corresponding elements of both ordered pairs are equal.\nNow compare (0, 12) to (0, -9). Let’s refer to these ordered pairs as\n$$ (0, 12) = (a, b) $$$$ (0, -9) = (c, d) $$Here, we see that while\n$$ a = c = 0 $$we also have that\n$$ b \\neq d $$because b = 12 and d = -9. Hence, we have that\n$$ (0, 12) \\neq (0, -9) $$as expected.\nOrdering Multiple Elements Naturally, we aren’t limited to two elements. We can collect as many elements as we want in a list. Equality of two n-tuples is also straightforward.\nn-Tuple, Equal An n-tuple is an ordered list of n elements and is normally written using parentheses as\n$$ (a_1, a_2,\\ldots, a_n) $$where $a_1$ is the first element in the n-tuple, $a_2$ is the second element, and so on as expected.\nTwo n-tuples $(a_1, a_2, \\ldots, a_n)$ and $(b_1, b_2, \\ldots, b_m)$ are called equal when they have the same number of elements (n = m), and their corresponding elements are equal to each other; that is, we have that\n$$ (a_1, a_2, \\ldots, a_n) = (b_1, b_2, \\ldots, b_m) $$if (and only if)\n$$ n = m $$and\n\\[ \\begin{array}{ c } a_1 = b_1 \\\\ a_2 = b_2 \\\\ \\vdots \\\\ a_n = b_m \\end{array} \\] It is common to refer to n-tuples with only two elements simply as tuples (in addition to ordered pairs), and n-tuples with three elements as triples (or ordered triples).\nExample 5.1.2 None of the following n-tuples are equal to each other because the order in which the elements appear is different between each ordered pair:\n\\[ \\begin{array}{ l l l} (1, 2, 3) \u0026 (2, 1, 3) \u0026 (3, 1, 2) \\\\ (1, 3, 2) \u0026 (2, 3, 1) \u0026 (3, 2, 1) \\end{array} \\]The following n-tuples are not equal to each other because they have a different number of elements:\n\\[ \\begin{array}{ l l } (1, 2, 3) \u0026 (1, 1, 2, 3) \\end{array} \\]",
    "description": "Just as we saw how sets can be built via propositional logic, we’ll see how relations and functions are built up using sets, a clear example of how multiple seemingly disparate concepts can be accumulated into a cohesive whole. But before we delve into what relations and functions are exactly, there is one more set operation we should get acquainted with: the set product. Ironically, even though order doesn’t matter when considering elements in a set, constructing a set product does require that we know in what order we are doing things. In this section, we discuss a new type of mathematical object that takes order into account.",
    "tags": [],
    "title": "Ordered Pairs",
    "uri": "/the-book-of-foundational-mathematics/relations/ordered-pairs/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Probability",
    "content": "This is a new chapter.",
    "description": "This is a new chapter.",
    "tags": [],
    "title": "Conditional Probability and Independence",
    "uri": "/the-book-of-probability/conditional-probability-and-independence/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics",
    "content": "This is a new chapter.",
    "description": "This is a new chapter.",
    "tags": [],
    "title": "Arguments and Proof",
    "uri": "/the-book-of-foundational-mathematics/arguments-and-proof/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Logic",
    "content": "In Section 1.1, having to write out all logical expressions in English and keeping track of truth values can be tedious and time-consuming. It also makes it difficult to evaluate the truth value of compound statements involving many propositions and logical connectives.\nIn analyzing a compound statement’s truth value, we need to be able to evaluate it for all combinations of truth values for its constituent parts. Without any organization, this can be quite error-prone. For example, to analyze the compound statement $p \\land q$, we have to check four cases: $p$ and $q$ are both false, $p$ is false and $q$ is true, $p$ is true and $q$ is false, and both $p$ and $q$ are true. Without any systematic way of dealing with each case, mistakes are easy to make.\nIn this section, we’ll describe a way to graphically organize our work.\nOrganizing Logical Expressions The first thing we can do to make things a bit easier is to simplify the logical values. Up to now, propositions have been evaluated as $\\text{true}$ or $\\text{false}$. Instead, we can make the following substitutions:\n\\[ \\begin{align*} \u00260\\ \\text{in place of false}\\\\ \u00261\\ \\text{in place of true} \\end{align*} \\]This let’s us express propositions using numerical values. For example, if $p$ evaluates to $\\text{true}$, we could instead say $p = 1$. Likewise, since $p$ is $\\text{true}$, that means that $\\neg p$ is $\\text{false}$, which is the same as saying $\\neg p = 0$.\nNext, we can organize groups of propositions into tables, and record the truth value of each proposition in a cell of that table. Such tables are called truth tables. In these types of tables, the first row is used to label each column with a proposition. Subsequent rows of these tables are used to record the numerical value of the propositions used in their respective columns.\nLet’s see an example of how such a table is constructed.\nExample 1.2.1 Suppose we want to evaluate the truth value for the proposition $\\neg p \\lor q$. We don't know whether or not $p$ or $q$ are primitive or compound, but it doesn't matter because either way, $p$ and $q$ are either going to be $\\text{true}$ or $\\text{false}$. $$~$$ We start by noting that the compound proposition $\\neg p \\lor q$ is a combination of the simpler propositions $p$ and $q$. Thus, we have to test the truth value of $\\neg p \\lor q$ by testing combinations of truth values for $p$ and $q$. There are four combinations of truth values taken by $p$ and $q$. $$~$$ Also notice we need to know $\\neg p$ before we know the value of $\\neg p \\lor q$, so we'll want to keep track of $\\neg p$ as well. $$~$$ So far, we've listed four propositions we want to track: $p,\\ q,\\ \\neg p,\\ \\neg p \\lor q$. Thus, we'll make a table with four columns, one for each of the propositions we're tracking. We already know that we essentially have to check all four combinations of truth values for $p$ and $q$, so we'll fill those combinations of values in to start: Notice that we separated the propositions $p$ and $q$ from $\\neg p$ and $\\neg p \\lor q$ with a thicker line. This is a stylistic choice that makes it easier to see which propositions we are setting values for, as opposed to propositions whose truth values we have to calculate. We also used a dark gray color for the row containing the column's proposition. We used a lighter gray color for the base proposition truth values because those are being set, and not calculated. $$~$$ Let's take a look at the row where $p = 1$ and $q = 0$, and the column for $\\neg p$. We'll color that cell green to draw our attention to it. Since $p = 1$, we know that $\\neg p = 0$, so we'll put a $0$ in that cell (the value of $q$ has no affect on this cell). Now we focus on the other cell in the $p = 1$, $q = 0$ row. Here the proposition being evaluated is $\\neg p \\lor q$. Since $\\neg p = 0$ and $q = 0$, we have by definition of disjunction that $\\neg p \\lor q = 0$. Following a similar process, we can fill in the missing truth values for the $p = 0,\\ q = 1$ row of the truth table: We fill out the remaining blank cells by simply evaluating the needed propositions using that row's values of $p$ and $q$: We aren't interested in the values of $\\neg p$, we only used them to help us figure out $\\neg p \\lor q$, so the column for $\\neg p$ can be removed, and we are left with the final truth table: Revisiting the Logical Connectives We can use truth tables to evaluate the logical connectives we previously talked about.\nNegation is perhaps the simplest logical operation:\nConjunction (and) and disjunction (inclusive-or) act on two propositions, so we’ll need four rows to examine all possible truth values.\nFinally, we also show the truth tables for the logical implications.\nWe could collect all of the above truth tables into one larger conglomerate truth table.",
    "description": "In Section 1.1, having to write out all logical expressions in English and keeping track of truth values can be tedious and time-consuming. It also makes it difficult to evaluate the truth value of compound statements involving many propositions and logical connectives.\nIn analyzing a compound statement’s truth value, we need to be able to evaluate it for all combinations of truth values for its constituent parts. Without any organization, this can be quite error-prone. For example, to analyze the compound statement $p \\land q$, we have to check four cases: $p$ and $q$ are both false, $p$ is false and $q$ is true, $p$ is true and $q$ is false, and both $p$ and $q$ are true. Without any systematic way of dealing with each case, mistakes are easy to make.",
    "tags": [],
    "title": "Modeling Logic With Truth Tables",
    "uri": "/the-book-of-foundational-mathematics/logic/modeling-logic-with-truth-tables/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Functions",
    "content": "We have defined functions basically as special types of relations. We can go further and recognize that some functions have special properties different than other functions. In this section, we examine three properties that tend to be useful when working on various problems all throughout mathematics.\nSurjective Functions Example 6.2.1 Consider the sets $A = \\{1, 2, 3, 4, 5\\}$ and $B = \\{\\alpha, \\beta, \\gamma, \\delta\\}$. In Figure 6.2.1 below, we use an arrow diagram to define two functions $f_1$ and $f_2$ from $A$ to $B$. Figure 6.2.1\nOne nice property of the function depicted in Figure 6.2.1 (a) is that we don't have to worry about which elements in the codomain are left out. In (b), we do have to keep track, which can be tricky to work with. Another way to think about this situation is that we are always guaranteed to be able to produce a preimage for every element in $B$ with the function in (a), whereas we do not have such a guarantee with the function in (b). The function depicted in Figure 6.2.1 (a) uses every element in its codomain, whereas the function depicted in (b) does not. We often find ourselves in situations where we need to be able to generate preimages under a function for specific elements (something we’ll need to do later in this chapter). Thus, having that guarantee will prove to be useful.\nSurjective, Onto Let $f: A \\rightarrow B$ be an arbitrary function.\nWe say that $f$ is a surjective function if (and only if) every element in $B$ is the image of at least one element $a \\in A$.\nIn other words,\n$$\\forall b \\in B\\ \\exists a \\in A\\ [b = f(a)]$$Sometimes, we call such a function onto.\nExample 6.2.2 Reconsider the function depicted in Figure 6.2.1 (a) again. The arrow diagram is enough to prove that $f_1$ is surjective, but we can also appeal to the definition as well. We need to check if\n$$\\forall b \\in B\\ \\exists a \\in A\\ [b = f_1(a)] = 1$$So, let’s check each element $b \\in B$, and see if there exists some element $a \\in A$ such that $b = f_1(a)$:\n$\\alpha = f_1(1)$ (we also have that $\\alpha = f_1(3)$, but we only need one preimage) $\\beta = f_1(5)$ $\\gamma = f_1(4)$ $\\delta = f_1(2)$ Every element in $\\{\\alpha, \\beta, \\gamma, \\delta\\}$ has a preimage under $f_1$, meaning $f_1$ is surjective.\nExample 6.2.3 Reconsider the function shown in Figure 6.2.1 (b). Let’s make a new function $f_3$ using those same mappings but let’s only use subsets of the domain and codomain:\n$$f_3: \\{1, 3, 4, 5\\} \\rightarrow \\{\\alpha, \\gamma, \\delta\\}$$where\n\\[ \\begin{align*} f_3(1) \u0026= \\alpha \\\\ f_3(3) \u0026= \\alpha \\\\ f_3(4) \u0026= \\gamma \\\\ f_3(5) \u0026= \\delta \\end{align*} \\]Even though the function shown in Figure 6.2.1 (b) is not surjective, we were able to form a surjective function using the same mappings for the elements $\\{1, 3, 4, 5\\}$. We did this by choosing the subset $\\{\\alpha, \\gamma, \\delta\\}$ of the original function’s codomain to act as the new function’s codomain.\nExample 6.2.4 Reconsider the function shown in Figure 6.2.1 (b). Let’s make a new function $f_4$ using those same mappings but unlike in Example 6.2.3, we’ll use a superset of the domain {1, 2, 3, 4, 5}. We’ll use the same codomain:\n$$f_4: \\{1, 2, 3, 4, 5, 6\\} \\rightarrow \\{\\alpha, \\beta, \\gamma, \\delta\\}$$where\n\\[ \\begin{align*} f_4(1) \u0026= \\alpha \\\\ f_4(2) \u0026= \\delta \\\\ f_4(3) \u0026= \\alpha \\\\ f_4(4) \u0026= \\gamma \\\\ f_4(5) \u0026= \\delta \\\\ f_4(6) \u0026= \\beta \\end{align*} \\]Even though the function shown in Figure 6.2.1 (b) is not surjective, we were able to form a surjective function using the same mappings for the elements $\\{1, 2, 3, 4, 5\\}$. We did this by ensuring that the new element added to the domain got mapped to the element not used in the original function.\nAs demonstrated in Example 6.2.3 and Example 6.2.4, the sets that make up a function’s domain and codomain play a role in determining whether or not that function is surjective or not. The same is true for the next property of functions we examine.\nInjective Functions Example 6.2.5 Consider the sets $A = \\{1, 2, 3, 4, 5\\}$ and $B = \\{\\alpha, \\beta, \\gamma, \\delta, \\varepsilon, \\zeta\\}$ In Figure 6.2.2 below, we depict two more functions from $A$ to $B$ using arrow diagrams. Figure 6.2.2\nLook at the function depicted in (b). Suppose we needed to pick one of the preimages of $\\zeta$. Do we choose $3$, or do we choose $4$? Because there is more than one preimage, it's not clear which one should be taken if we are ever in a situation where we needed to choose (again, this will happen later in this chapter) unless we had some additional information. We don't have this problem with the function in (a) since there is only ever at most one preimage for each element in $B$. As noted in Example 6.2.5, we can run into problems if different elements in $A$ get mapped to the same element in $B$ because choosing specific preimages now becomes ambiguous. However, if there is never more than one preimage for each element in $B$, then the choice is automatic.\nInjective, One-to-one Let $f: A \\rightarrow B$ be an arbitrary function.\nWe call $f$ an injective function if (and only if) every element in $B$ has no more than one preimage in $A$.\nLogically, we write this as\n$$\\forall a_1, a_2 \\in A\\ [f(a_1) = f(a_2) \\implies a_1 = a_2]$$Often, we describe such functions as being one-to-one.\nInitially, the quantified proposition looks like it has nothing to with saying that every element in a function’s codomain has no more than one preimage, but the reason why we use this as the definition will be clearer after looking at some examples.\nExample 6.2.6 Let’s look at the function depicted in Figure 6.2.2 (a). Visually, we see that no element in $B$ has more than one preimage. Let’s go through and quickly count the number of preimages as well:\n$\\alpha \\in B$ has $1$ preimage $\\color{\\green}{\\checkmark}$ $\\beta \\in B$ has $1$ preimage $\\color{\\green}{\\checkmark}$ $\\gamma \\in B$ has $1$ preimage $\\color{\\green}{\\checkmark}$ $\\delta \\in B$ has $0$ preimages $\\color{\\green}{\\checkmark}$ $\\varepsilon \\in B$ has $1$ preimage $\\color{\\green}{\\checkmark}$ $\\zeta \\in B$ has $1$ preimage $\\color{\\green}{\\checkmark}$ The function depicted in (a) is injective.\nWe can do the same thing for the function depicted in (b):\n$\\alpha \\in B$ has $1$ preimage $\\color{\\green}{\\checkmark}$ $\\beta \\in B$ has $1$ preimage $\\color{\\green}{\\checkmark}$ $\\gamma \\in B$ has $1$ preimage $\\color{\\green}{\\checkmark}$ $\\delta \\in B$ has $0$ preimages $\\color{\\green}{\\checkmark}$ $\\varepsilon \\in B$ has $0$ preimages $\\color{\\green}{\\checkmark}$ $\\zeta \\in B$ has $2$ preimages $\\color{\\red}{\\times}$ Thus, the function depicted in (b) is not injective.\nExample 6.2.7 Now, let’s use the quantified proposition to determine if each of the functions shown in Figure 6.2.2 are injective.\na.) Figure 6.2.2 (a) We need to check all pairwise elements in $A$, and ensure the necessary implications are all true. Just like we did when examining relations, we’ll use a modified truth table to organize our checking. First, we’ll use the first two columns to list each pair of elements from $A$. The next two columns will show the images of the elements listed in the first two columns. The remaining columns will show the values of the necessary propositions.\nWe won’t list rows like $a_1 = 3,\\ a_2 = 1$ since that will be the exact same row as $a_1 = 1,\\ a_2 = 3$. We eliminate a lot of redundant rows in the table by doing this.\n$a_1$ $a_2$ $g_1(a_1)$ $g_1(a_2)$ $g_1(a_1) = g_1(a_2)$ $a_1 = a_2$ $g_1(a_1) = g_1(a_2) \\rightarrow a_1 = a_2$ 1 1 $\\beta$ $\\beta$ 1 1 1 1 2 $\\beta$ $\\alpha$ 0 0 1 1 3 $\\beta$ $\\zeta$ 0 0 1 1 4 $\\beta$ $\\varepsilon$ 0 0 1 1 5 $\\beta$ $\\gamma$ 0 0 1 2 2 $\\alpha$ $\\alpha$ 1 1 1 2 3 $\\alpha$ $\\zeta$ 0 0 1 2 4 $\\alpha$ $\\varepsilon$ 0 0 1 2 5 $\\alpha$ $\\gamma$ 0 0 1 3 3 $\\zeta$ $\\zeta$ 1 1 1 3 4 $\\zeta$ $\\varepsilon$ 0 0 1 3 5 $\\zeta$ $\\gamma$ 0 0 1 4 4 $\\varepsilon$ $\\varepsilon$ 1 1 1 4 5 $\\varepsilon$ $\\gamma$ 0 0 1 5 5 $\\gamma$ $\\gamma$ 1 1 1 We see that all of the truth values in the last column are all 1s, which means that for every pair of elements in $A$, the necessary implication is true. Thus, by definition, the function shown in Figure 6.2.2 (a) is injective.\nNotice that when the two function evaluations were equal (shown in the fifth column), the inputs to those functions were always referring to the same element, and when those function evaluations were different, the inputs were also different.\nLook closely at the rows where $a_1 = a_2$. Even though they are different variables, they are still referring to the same element in $B$. This is why we phrased the definition of injective function the way we did. We want to ensure that if two inputs to a function result in the same image, then those two inputs are referring to the same element. In other words, only that one element in $A$ produces that specific element from $B$.\nb.) Figure 6.2.2 (b) We’ll construct one more table similar to the one above for $g_2$:\n$a_1$ $a_2$ $g_2(a_1)$ $g_2(a_2)$ $g_2(a_1) = g_2(a_2)$ $a_1 = a_2$ $g_2(a_1) = g_2(a_2) \\rightarrow a_1 = a_2$ 1 1 $\\beta$ $\\beta$ 1 1 1 1 2 $\\beta$ $\\alpha$ 0 0 1 1 3 $\\beta$ $\\zeta$ 0 0 1 1 4 $\\beta$ $\\zeta$ 0 0 1 1 5 $\\beta$ $\\gamma$ 0 0 1 2 2 $\\alpha$ $\\alpha$ 1 1 1 2 3 $\\alpha$ $\\zeta$ 0 0 1 2 4 $\\alpha$ $\\zeta$ 0 0 1 2 5 $\\alpha$ $\\gamma$ 0 0 1 3 3 $\\zeta$ $\\zeta$ 1 1 1 3 4 $\\zeta$ $\\zeta$ 1 0 0 3 5 $\\zeta$ $\\gamma$ 0 0 1 4 4 $\\zeta$ $\\zeta$ 1 1 1 4 5 $\\zeta$ $\\gamma$ 0 0 1 5 5 $\\gamma$ $\\gamma$ 1 1 1 Here, not all truth values in the last column are 1, thus $g_2$ is not injective. Look at the row corresponding to $a_1 = 3, a_2 = 4$. We see that $a_1 \\neq a_2$, yet $g_2(a_1) = g_2(a_2) = \\zeta$.\nOne thing we notice with $a_1 = a_2 = 3$, we also have that $g_2(a_1) = g_2(a_2) = \\zeta$. This is expected since $a_1$ and $a_2$ are referring to the same element, even though they are different variables.\nBijective Functions We’ve already talked about two properties a function can have: being surjective, and being injective. Naturally, we may ask whether or not a function can be both surjective and injective at the same time.\nExample 6.2.7 Consider the sets $A = \\{1, 2, 3, 4, 5\\}$ and $B = \\{\\alpha, \\beta, \\gamma, \\delta, \\varepsilon\\}$ In Figure 6.2.3 below, we depict the functions $h_1: A \\rightarrow B$ in (a) and $h_2: A \\rightarrow B$ in (B) using arrow diagrams. Figure 6.2.3\nHere, we see that $h_1$ is both surjective and injective, while $h_2$ is neither surjective nor injective. Bijective Let $f: A \\rightarrow B$ be an arbitrary function.\nWe call $f$ a bijective function if (and only if) $f$ is surjective and injective.\nWe’ve seen examples of functions that are neither surjective nor injective, functions that are one without being the other, and one function that is both.\nExample 6.2.8 From Figure 6.2.1, we see that $f_1$ is surjective, but it is not injective, meaning $f_1$ is not bijective. $f_2$ is neither surjective nor injective, and is also not bijective.\nFrom Figure 6.2.2, we see that $g_1$ is injective, but it is not surjective, and so $g_1$ is not bijective. We also see that $g_2$ is neither injective nor surjective, so it isn’t bijective either.\nBecause bijective functions have both of these really nice properties, they will be especially helpful to us later in this chapter, and in later chapters.\nIt is common to refer to a bijective function as a one-to-one correspondence, and you’ll see this in many texts. However, this sounds very similar to one-to-one, which refers to injective functions, not bijective functions. We avoid the term one-to-one correspondence in this book to mitigate any potential confusion.",
    "description": "We have defined functions basically as special types of relations. We can go further and recognize that some functions have special properties different than other functions. In this section, we examine three properties that tend to be useful when working on various problems all throughout mathematics.\nSurjective Functions Example 6.2.1 Consider the sets $A = \\{1, 2, 3, 4, 5\\}$ and $B = \\{\\alpha, \\beta, \\gamma, \\delta\\}$. In Figure 6.2.1 below, we use an arrow diagram to define two functions $f_1$ and $f_2$ from $A$ to $B$. Figure 6.2.1",
    "tags": [],
    "title": "Special Functions",
    "uri": "/the-book-of-foundational-mathematics/functions/special-functions/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Relations",
    "content": "All of the set operations we’ve seen so far simply take some combination of elements from two given sets, and form a new conglomerate set. The same will be true of Cartesian Products.\nCartesian Products For all of the set operations we’ve seen so far, we essentially pick out only a subset of elements from each. What’s different about the Cartesian Product is that we iterate over all elements from both sets.\nCartesian Product, Set Product, Cross Product Let A and B be any two given sets.\nThe Cartesian Product of A and B, denoted\n$$ A \\times B $$is the set\n$$ A \\times B = \\{ (a, b)\\ |\\ a \\in A\\ \\land\\ b \\in B \\} $$Note that A and B do not need to be subsets from the same universe.\nSometimes, this is referred to as a Set Product, or a Cross Product.\nBasically, for each element in A, we iterate over all of the elements in B, forming ordered pairs where the element from A is the first item in the pair, and the element from B is the second item in the pair.\nWe can organize the construction of the Cartesian Product in a table like so:\n$A \\times B$ $b_1$ $b_2$ $\\ldots$ $b_m$ $a_1$ $(a_1, b_1)$ $(a_1, b_2)$ $\\cdots$ $(a_1, b_m)$ $a_2$ $(a_2, b_1)$ $(a_2, b_2)$ $\\cdots$ $(a_2, b_m)$ $\\vdots$ $\\vdots$ $\\vdots$ $\\ddots$ $\\vdots$ $a_n$ $(a_n, b_1)$ $(a_n, b_2)$ $\\cdots$ $(a_n, b_m)$ Remember that since we’re forming ordered pairs, the order in which we list the sets is also important.\nExample 5.2.1 Suppose we have\n$$A = \\{1, 2\\}$$$$B = \\{a, b, c\\}$$where a, b, c are not variables, but simply the first, second, and third letters of the English alphabet. Then we have that\n$$A \\times B = \\{ (1, a),\\ (1, b),\\ (1, c),\\ (2, a),\\ (2, b),\\ (2, c) \\}$$On the other hand, we have that\n$$B \\times A = \\{ (a, 1),\\ (a, 2),\\ (b, 1),\\ (b, 2),\\ (c, 1),\\ (c, 2) \\}$$We can use tables to verify that these are the correct Cartesian Products. For example, here’s the table for $A \\times B$:\n$A \\times B$ a b c 1 (1, a) (1, b) (1, c) 2 (2, a) (2, b) (2, c) Here’s the table for $B \\times A$:\n$B \\times A$ 1 2 a (a, 1) (a, 2) b (b, 1) (b, 2) c (c, 1) (c, 2) The fact that we can’t swap the order of the sets without potentially affecting the result deserves extra emphasis.\nOrder Matters in Cartesian Products In general, we have that\n$$A \\times B \\neq B \\times A$$ We are not limited to two sets.\nCartesian Products with Multiple Sets Just as we can take the product of multiple numbers, we can also take the Cartesian Product of multiple sets.\nMultiple Cartesian Product Let $A_1$, $A_2$, $\\ldots$, $A_n$ be arbitrary sets.\nThe Multiple Cartesian Product\n$$A_1 \\times A_2 \\times \\cdots \\times A_n$$is the set\n$$\\{(a_1,\\ a_2,\\ \\ldots,\\ a_n)\\ |\\ a_1 \\in A_1\\ \\land\\ a_2 \\in A_2\\ \\land\\ \\cdots\\ \\land\\ a_n \\in A_n\\}$$ Oftentimes, we refer to a Multiple Cartesian Product simply as a Cartesian Product.\nExample 5.2.2 Consider the following sets:\n\\[ \\begin{array}{ l c r } A = \\{1, 2\\} \u0026 B = \\{a, b\\} \u0026 C = \\{\\alpha, \\beta\\} \\end{array} \\]Then we have by definition that\n$$ A \\times B \\times C = \\{(1, a, \\alpha), (1, a, \\beta), (1, b, \\alpha), (1, b, \\beta), (2, a, \\alpha), (2, a, \\beta), (2, b, \\alpha), (2, b, \\beta) \\} $$Of course, swapping the order of A, B, C would yield slightly different sets, such as\n$$ B \\times A \\times C = \\{(a, 1, \\alpha), (a, 1, \\beta), (a, 2, \\alpha), (a, 2, \\beta), (b, 1, \\alpha), (b, 1, \\beta), (b, 2, \\alpha), (b, 2, \\beta) \\} $$ Though the examples we’ve seen all use different sets, we can use just one set multiple times.\nRepeated Cartesian Products Just as a number can be multiplied by itself multiple times, so too can sets. We use exponents to represent the number of times we are taking the Cartesian Product of a set with itself. Suppose that A represents some arbitrary set, the notation\n$$A^n$$is shorthand for the Cartesian Product\n$$\\underbrace{A \\times A \\times \\cdots \\times A}_{\\text{n times}}$$ Example 5.2.3 Consider the set\n$$A = \\{1,\\ a\\}$$We have that\n\\[ \\begin{align*} A^2 \u0026= A \\times A \\\\ \u0026= \\{(1, 1),\\ (1, a),\\ (a, 1),\\ (a, a)\\} \\end{align*} \\]We also have that\n\\[ \\begin{align*} A^3 \u0026= A \\times A \\times A \\\\ \u0026= \\{(1, 1, 1),\\ (1, 1, a),\\ (1, a, 1),\\ (1, a, a), (a, 1, 1),\\ (a, 1, a),\\ (a, a, 1),\\ (a, a, a)\\} \\end{align*} \\]",
    "description": "All of the set operations we’ve seen so far simply take some combination of elements from two given sets, and form a new conglomerate set. The same will be true of Cartesian Products.\nCartesian Products For all of the set operations we’ve seen so far, we essentially pick out only a subset of elements from each. What’s different about the Cartesian Product is that we iterate over all elements from both sets.",
    "tags": [],
    "title": "Cartesian Products",
    "uri": "/the-book-of-foundational-mathematics/relations/cartesian-products/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Logic",
    "content": "Now that we have a fairly organized way of analyzing the truth value of compound propositions, we’ll start to see some interesting phenomena crop up from time-to-time. We’ll do this by looking at all possible values a proposition can take, which is precisely what truth tables allow us to do.\nTautologies Example 1.3.1 Let's examine the proposition $p \\rightarrow (p \\lor q)$. We'll start with a blank truth table. We start by filling out the column for $p \\lor q$: Now that we have the truth values for propositions $p$ and $p \\lor q$, it is easy to calculate the truth values for $p \\rightarrow (p \\lor q)$: Now we remove the extraneous column for $p \\lor q$ to get the final truth table: All of the values in the final column are $1$! This means that, no matter what truth values $p$ and $q$ have, the expression $p \\rightarrow (p \\lor q)$ is always $\\text{true}$. $$~$$ Translating the proposition $p \\rightarrow (p \\lor q)$ into English yields something like this: $$\\text{If p, then p or q.}$$ Of course if we start with proposition $p$, then we must also have $p \\lor q$ as well because $p$ is included in the conclusion. For example, suppose we had that \\[ \\begin{align*} p \u0026: \\text{Phil opens up a new savings account.} \\\\ q \u0026: \\text{Phil earns 4% interest every year, compounded monthly.} \\end{align*} \\] Then the proposition $p \\rightarrow (p \\lor q)$ can be translated as follows: \\[ \\begin{align*} \u0026\\text{If Phil opens up a new savings account, then} \\\\ \u0026\\text{either Phil opens up a new savings account, or } \\\\ \u0026\\text{he will earn 4% interest every year compounded monthly, } \\\\ \u0026\\text{or both.} \\end{align*} \\] $$~$$ Of course if Phil opens a new savings account, then he will either open up a new savings account (which he just supposedly did), or he will earn 4% interest compounded monthly, or both. There is a special name for propositions that are always true, no matter what truth values are held by its constituent parts.\nTautology A proposition is called a tautology if it is true for all possible truth value assignments for its component parts.\nOften, the symbol $T_0$ is used to represent tautological propositions.\nContradictions As observed above, some truth tables have columns whose only value is $1$. Of course, we could run into the opposite situation.\nExample 1.3.2 Suppose we wanted to examine the proposition $p \\land (\\neg p \\land q)$. Naturally, we start with an empty truth table. We propositions $\\neg p$ and $\\neg p \\land q$ are relatively easy to calculate. Once those propositions' columns are filled out, then filling out the final column becomes straightforward: Finally, we'll remove the columns for any propositions we don't care about: Every value in the last column is $0$, meaning no matter what combination of truth values we have for $p$ and $q$, the proposition $p \\land (\\neg p \\land q)$ will always be $\\text{false}$. Then again, perhaps we shouldn't be surprised. After all, how can both $p$ and $\\neg p$ be true? To make this situation more concrete, take another look at the propositions we used in Example 1.3.1: \\[ \\begin{align*} p \u0026: \\text{Phil opens up a new savings account.} \\\\ q \u0026: \\text{Phil earns 4% interest every year, compounded monthly.} \\end{align*} \\] With these propositions, we can translate $p \\land (\\neg p \\land q)$ into something like the following: \\[ \\begin{align*} \u0026\\text{Phil opens up a new savings account, and} \\\\ \u0026\\text{he does not open up a new savings account, and} \\\\ \u0026\\text{he will earn 4% interest every year compounded monthly.} \\end{align*} \\] How can Phil open up a new savings account while simultaneously not opening up a new savings account? This is an impossible scenario. We have a different term for the sorts of situations depicted in Example 1.3.2.\nContradiction A proposition is called a contradiction if it is false for all possible truth value assignments for its component parts.\nOften, the symbol $F_0$ is used to represent contradictory propositions.\nSatisfiability For a given compound proposition, a truth table allows us to quickly look for any truth value assignments that yield a truth value of $1$ or $0$.",
    "description": "Now that we have a fairly organized way of analyzing the truth value of compound propositions, we’ll start to see some interesting phenomena crop up from time-to-time. We’ll do this by looking at all possible values a proposition can take, which is precisely what truth tables allow us to do.\nTautologies Example 1.3.1 Let's examine the proposition $p \\rightarrow (p \\lor q)$. We'll start with a blank truth table. We start by filling out the column for $p \\lor q$: Now that we have the truth values for propositions $p$ and $p \\lor q$, it is easy to calculate the truth values for $p \\rightarrow (p \\lor q)$: Now we remove the extraneous column for $p \\lor q$ to get the final truth table: All of the values in the final column are $1$! This means that, no matter what truth values $p$ and $q$ have, the expression $p \\rightarrow (p \\lor q)$ is always $\\text{true}$. $$~$$ Translating the proposition $p \\rightarrow (p \\lor q)$ into English yields something like this: $$\\text{If p, then p or q.}$$ Of course if we start with proposition $p$, then we must also have $p \\lor q$ as well because $p$ is included in the conclusion. For example, suppose we had that \\[ \\begin{align*} p \u0026: \\text{Phil opens up a new savings account.} \\\\ q \u0026: \\text{Phil earns 4% interest every year, compounded monthly.} \\end{align*} \\] Then the proposition $p \\rightarrow (p \\lor q)$ can be translated as follows: \\[ \\begin{align*} \u0026\\text{If Phil opens up a new savings account, then} \\\\ \u0026\\text{either Phil opens up a new savings account, or } \\\\ \u0026\\text{he will earn 4% interest every year compounded monthly, } \\\\ \u0026\\text{or both.} \\end{align*} \\] $$~$$ Of course if Phil opens a new savings account, then he will either open up a new savings account (which he just supposedly did), or he will earn 4% interest compounded monthly, or both. There is a special name for propositions that are always true, no matter what truth values are held by its constituent parts.",
    "tags": [],
    "title": "Always True, Always False",
    "uri": "/the-book-of-foundational-mathematics/logic/always-true-always-false/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Functions",
    "content": "This is red colored text {style=“color: red;”}",
    "description": "This is red colored text {style=“color: red;”}",
    "tags": [],
    "title": "Properties of Functions",
    "uri": "/the-book-of-foundational-mathematics/functions/properties-of-functions/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics",
    "content": "This is a new chapter.",
    "description": "This is a new chapter.",
    "tags": [],
    "title": "Set Theory",
    "uri": "/the-book-of-foundational-mathematics/set-theory/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Relations",
    "content": "The Cartesian Product of two sets is just another set. This means we can also use other set operations like union, intersection, and set differences.\nIn this section, we explore how the Cartesian Product interacts with the other set operators.\nAssociativity of the Cartesian Product We already discussed how order matters in Cartesian Products, and as such sets don’t commute around the Cartesian Product operator $\\times$. Associativity is another common property we should examine. What exactly is the difference between\n\\[ \\begin{array}{ l c r} \\underline{(A \\times B) \\times C} \u0026 \\underline{A \\times B \\times C} \u0026 \\underline{A \\times (B \\times C)} \\end{array} \\]if any difference exists at all?\nRemember that parentheses are used to indicate which operations are to be performed first.\nExample 5.3.1 Consider the sets\n$$A = \\{1,\\ 2\\}$$$$B = \\{a,\\ b\\}$$$$C = \\{\\Psi,\\ \\Omega\\}$$(Note that $\\Psi$ and $\\Omega$ are letters from the Greek alphabet, and are being used here simply for the sake of variety.)\nFrom the previous section, we already know that\n$$A \\times B \\times C = \\{(1, a, \\Psi),\\ (1, a, \\Omega),\\ (1, b, \\Psi),\\ (1, b, \\Omega),\\ (2, a, \\Psi),\\ (2, a, \\Omega),\\ (2, b, \\Psi),\\ (2, b, \\Omega)\\}$$However, to compute $(A \\times B) \\times C$ and $A \\times (B \\times C)$, we must remember to compute what’s in parentheses first.\nComputing $(A \\times B) \\times C$ First, we must compute $A \\times B$ as follows:\n$$A \\times B = \\{(1, a),\\ (1, b),\\ (2, a),\\ (2, b)\\}$$Finally, we can now compute $(A \\times B) \\times C$ like so:\n\\[ \\begin{align*} (A \\times B) \\times C \u0026= \\{(1, a),\\ (1, b),\\ (2, a),\\ (2, b)\\} \\times \\{\\Psi,\\ \\Omega\\} \\\\ \u0026= \\{((1, a), \\Psi),\\ ((1, a), \\Omega),\\ ((1, b), \\Psi),\\ ((1, b), \\Omega),\\ ((2, a), \\Psi),\\ ((2, a), \\Omega),\\ ((2, b), \\Psi),\\ ((2, b), \\Omega)\\} \\end{align*} \\]Notice that each element of $(A \\times B) \\times C$ is an ordered pair, not an ordered triple like with $A \\times B \\times C$. Furthermore, each ordered pair in $(A \\times B) \\times C$ contains an ordered pair.\nBecause all of the parentheses can be confusing to look at, let’s list out all of the elements in $(A \\times B) \\times C$ in a grid:\n\\[ \\begin{array}{ c c} ((1, a), \\Psi) \u0026 ((1, a), \\Omega) \\\\ ((1, b), \\Psi) \u0026 ((1, b), \\Omega) \\\\ ((2, a), \\Psi) \u0026 ((2, a), \\Omega) \\\\ ((2, b), \\Psi) \u0026 ((2, b), \\Omega) \\\\ \\end{array} \\]Computing $A \\times (B \\times C)$ Computing $A \\times (B \\times C)$ is similar, except we of course start by computing $B \\times C$:\n\\[ \\begin{align*} A \\times (B \\times C) \u0026= \\{1, 2\\} \\times \\{(a, \\Psi),\\ (a, \\Omega),\\ (b, \\Psi),\\ (b, \\Omega)\\} \\\\ \u0026= \\{(1, (a, \\Psi)),\\ (1, (a, \\Omega)),\\ (1, (b, \\Psi)),\\ (1, (b, \\Omega)),\\ (2, (a, \\Psi)),\\ (2, (a, \\Omega)),\\ (2, (b, \\Psi)),\\ (2, (b, \\Omega))\\} \\end{align*} \\] Just as order matters when writing a Cartesian Product, the way we associate sets using parentheses in a Cartesian Product also matters, as demonstrated in Example 5.3.1. This deserves special emphasis.\nAssociation Matters with Cartesian Products The Cartesian Products\n\\begin{array}{ c c c } (A \\times B) \\times C \u0026 A \\times B \\times C \u0026 A \\times (B \\times C) \\end{array}\nare not equal to one another.\nBased on the above discussion, it should come as no surprise that\n$$A^m \\times A^n \\neq A^{m + n}$$Instead, $A^m \\times A^n$ is shorthand for\n$$(\\underbrace{A \\times A \\times \\cdots \\times A}_{\\text{m times}}) \\times (\\underbrace{A \\times A \\times \\cdots \\times A}_{\\text{n times}})$$whereas $A^{m + n}$ is shorthand for\n$$\\underbrace{A \\times A \\times \\cdots \\times A}_{\\text{m + n times}}$$Cartesian Products with the Empty Set Before we formally prove the next theorem, let’s take a moment to think about what would happen if we took the Cartesian Product of an arbitrary set A and the Empty Set.\nFor example, for every element in A, the Cartesian Product of A and B involves pairing the element in A with every element in B. However, if there were no elements in B, then there would be nothing to pair with the elements in A. Thus, no ordered pairs could be formed, and the resulting Cartesian Product would be empty.\nTheorem 5.3.1 For any arbitrary set A, we have that\n$$A \\times \\emptyset = \\emptyset$$$$\\emptyset \\times A = \\emptyset$$ Proof 5.3.1 General Strategy: By assuming that $A \\times \\emptyset$ is not empty, that would imply that $\\ \\emptyset$ has at least one element, contradicting the definition of the Empty Set. The exact same strategy works for $\\ \\emptyset \\times A$ as well.\nPresume - for the purpose of showing a contradiction - that $A \\times \\emptyset$ is non-empty. This would mean that there is at least one element in $A \\times \\emptyset$. That element would be an ordered pair which we’ll denote (a, ?). As such, we know that\n$$(a, ?) \\in A \\times \\emptyset$$Now, by the definition of Cartesian Product, we then have that\n\\[ \\begin{align*} a \u0026\\in A \\\\ ? \u0026\\in \\emptyset \\end{align*} \\]However, the fact that $? \\in \\emptyset$ contradicts the fact that $|\\emptyset| = 0$.\nHence, our presupposition that $A \\times \\emptyset$ is non-empty led to a contradiction, meaning we must have that\n$$A \\times \\emptyset = \\emptyset$$as desired.\nShowing that $\\ \\emptyset \\times A = \\emptyset$ is nearly identical.\n$\\times$ Distributes Over $\\cup$ and $\\cap$ Here, we’ll see some of the distributive properties of the Cartesian Product.\nTheorem 5.3.2 For arbitrary sets A, B, C, we have that\n\\[ \\begin{align*} A \\times (B \\cup C) \u0026= (A \\times B) \\cup (A \\times C) \\\\ (A \\cup B) \\times C \u0026= (A \\times C) \\cup (B \\times C) \\end{align*} \\] Proof 5.3.2 General Strategy: We’ll prove that\n$$A \\times (B \\cup C) = (A \\times B) \\cup (A \\times C)$$by showing that each side is a subset of the other side. We’ll do this by using element arguments. The exact same logic also applies to showing that\n$$(A \\cup B) \\times C = (A \\times C) \\cup (B \\times C)$$so we’ll omit that part of the proof for the sake of brevity.\nStep 1: Show that $A \\times (B \\cup C) \\subseteq (A \\times B) \\cup (A \\times C)$ \\[ \\begin{array}{ r l l } \u0026 \\mathbf{(a, x) \\in A \\times (B \\cup C)} \u0026 \\textbf{Reason} \\\\ \\implies \u0026 (a \\in A) \\land (x \\in B \\cup C) \u0026 \\text{Definition of Cartesian Product} \\\\ \\implies \u0026 (a \\in A) \\land [(x \\in B) \\lor (x \\in C)] \u0026 \\text{Definition of Union} \\\\ \\implies \u0026 [(a \\in A) \\land (x \\in B)] \\lor [(a \\in A) \\land (x \\in C)] \u0026 \\text{Distribution of } \\land \\text{ over } \\lor \\\\ \\implies \u0026 [(a, x) \\in A \\times B] \\lor [(a, x) \\in A \\times C] \u0026 \\text{Definition of Cartesian Product} \\\\ \\implies \u0026 (a, x) \\in (A \\times B) \\cup (A \\times C) \u0026 \\text{Definition of Union} \\end{array} \\]Thus, we’ve just shown that whenever $(a, x) \\in A \\times (B \\cup C)$, we also have that $(a, x) \\in (A \\times B) \\cup (A \\times C)$. This means that we have by definition that\n$$A \\times (B \\cup C) \\subseteq (A \\times B) \\cup (A \\times C)$$Step 2: Show that $(A \\times B) \\cup (A \\times C) \\subseteq A \\times (B \\cup C)$ \\[ \\begin{array}{ r l l } \u0026 \\mathbf{(a, x) \\in (A \\times B) \\cup (A \\times C)} \u0026 \\textbf{Reason} \\\\ \\implies \u0026 [(a, x) \\in A \\times B] \\lor [(a, x) \\in A \\times C] \u0026 \\text{Definition of Union} \\\\ \\implies \u0026 [(a \\in A) \\land (x \\in B)] \\lor [(a \\in A) \\land (x \\in C)] \u0026 \\text{Definition of Cartesian Product} \\\\ \\implies \u0026 (a \\in A) \\land (x \\in B\\ \\lor\\ x \\in C) \u0026 \\text{Distribution of } \\land \\text{ of } \\lor \\\\ \\implies \u0026 (a \\in A) \\land (x \\in B \\cup C) \u0026 \\text{Definition of Union} \\\\ \\implies \u0026 (a, x) \\in A \\times (B \\cup C) \u0026 \\text{Definition of Cartesian Product} \\end{array} \\]Now we have shown that\n$$(A \\times B) \\cup (A \\times C) \\subseteq A \\times (B \\cup C)$$Conclusion: $A \\times (B \\cup C) = (A \\times B) \\cup (A \\times C)$ Because we’ve shown both\n$$A \\times (B \\cup C) \\subseteq (A \\times B) \\cup (A \\times C)$$$$(A \\times B) \\cup (A \\times C) \\subseteq A \\times (B \\cup C)$$we have, by definition of set equality, that\n$$A \\times (B \\cup C) = (A \\times B) \\cup (A \\times C)$$as desired!\nNotice that in the above proof that, even though mathematical definitions use $\\Longleftrightarrow$, we only considered one direction of the logical implication at a time (since we only used $\\implies$). That is why we had to beak the proof up into two steps along with a concluding step.\nHowever, with careful execution, we could have used $\\Longleftrightarrow$ to make the proof more compact.\nLet’s see an example of Theorem 5.3.2 in action.\nExample 5.3.2 Consider the sets\n\\[ \\begin{align*} A \u0026= \\{1, 2, 3\\} \\\\ B \u0026= \\{a, b\\} \\\\ C \u0026= \\{\\alpha, \\beta\\} \\end{align*} \\]According to Theorem 5.3.2, the two sets\n$$A \\times (B \\cup C)$$and\n$$(A \\times B) \\cup (A \\times C)$$should be equal. Let’s check.\nStep 1: Compute $A \\times (B \\cup C)$ First, we compute $B \\cup C$:\n\\[ \\begin{align*} B \\cup C \u0026= \\{a, b\\} \\cup \\{\\alpha, \\beta\\} \\\\ \u0026= \\{a, b, \\alpha, \\beta\\} \\end{align*} \\]Now we can compute $A \\times (B \\cup C)$:\n\\[ \\begin{align*} A \\times (B \\cup C) \u0026= \\{1, 2, 3\\} \\times \\{a, b, \\alpha, \\beta\\} \\\\ \u0026= \\{(1, a), (1, b), (1, \\alpha), (1, \\beta), (2, a), (2, b), (2, \\alpha), (2, \\beta), (3, a), (3, b), (3, \\alpha), (3, \\beta)\\} \\end{align*} \\]Let’s lay out all of these elements in a grid to get a better look:\n\\[ \\begin{array}{ c c c c } (1, a) \u0026 (1, b) \u0026 (1, \\alpha) \u0026 (1, \\beta) \\\\ (2, a) \u0026 (2, b) \u0026 (2, \\alpha) \u0026 (2, \\beta) \\\\ (3, a) \u0026 (3, b) \u0026 (3, \\alpha) \u0026 (3, \\beta) \\end{array} \\]Step 2: Compute $(A \\times B) \\cup (A \\times C)$ First, compute $A \\times B$:\n\\[ \\begin{align*} A \\times B \u0026= \\{1, 2, 3\\} \\times \\{a, b\\} \\\\ \u0026= \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\end{align*} \\]Next, we compute $A \\times C$:\n\\[ \\begin{align*} A \\times C \u0026= \\{1, 2, 3\\} \\times \\{\\alpha, \\beta\\} \\\\ \u0026= \\{(1, \\alpha), (1, \\beta), (2, \\alpha), (2, \\beta), (3, \\alpha), (3, \\beta)\\} \\end{align*} \\]Finally, we can compute $(A \\times B) \\cup (A \\times C)$:\n\\[ \\begin{align*} (A \\times B) \\cup (A \\times C) \u0026= \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\cup \\{(1, \\alpha), (1, \\beta), (2, \\alpha), (2, \\beta), (3, \\alpha), (3, \\beta)\\} \\\\ \u0026= \\{(1, a), (1, b), (1, \\alpha), (1, \\beta), (2, a), (2, b), (2, \\alpha), (2, \\beta), (3, a), (3, b), (3, \\alpha), (3, \\beta)\\} \\end{align*} \\]Just like before, let’s lay out all of these elements in a grid to get a better look:\n\\[ \\begin{array}{ c c c c } (1, a) \u0026 (1, b) \u0026 (1, \\alpha) \u0026 (1, \\beta) \\\\ (2, a) \u0026 (2, b) \u0026 (2, \\alpha) \u0026 (2, \\beta) \\\\ (3, a) \u0026 (3, b) \u0026 (3, \\alpha) \u0026 (3, \\beta) \\end{array} \\]As we can see, these are exactly the same elements we got from the Cartesian Product from Step 1. As such, we see that in this example, we have verified that\n$$A \\times (B \\cup C) = (A \\times B) \\cup (A \\times C)$$ In Theorem 5.3.2, we mentioned that A, B, C can be any arbitrary sets. This means any of the sets could be the empty set.\nExample 5.3.3 Consider the sets\n\\[ \\begin{align*} A \u0026= \\{1, 2, 3\\} \\\\ B \u0026= \\emptyset \\\\ C \u0026= \\{\\alpha, \\beta\\} \\end{align*} \\]Step 1: Compute $A \\times (B \\cup C)$ \\[ \\begin{align*} A \\times (B \\cup C) \u0026= A \\times (\\emptyset \\cup C) \\\\ \u0026= A \\times C \\\\ \u0026= \\{1, 2, 3\\} \\times \\{\\alpha, \\beta\\} \\\\ \u0026= \\{(1, \\alpha), (1, \\beta), (2, \\alpha), (2, \\beta), (3, \\alpha), (3, \\beta)\\} \\end{align*} \\]Step 2: Compute $(A \\times B) \\cup (A \\times C)$ According to Theorem 5.3.1, we have that $A \\times \\emptyset = \\emptyset$. This makes computing $(A \\times B) \\cup (A \\times C)$ much easier.\n\\[ \\begin{align*} (A \\times B) \\cup (A \\times C) \u0026= (A \\times \\emptyset) \\cup (A \\times C) \\\\ \u0026= \\emptyset \\cup (A \\times C) \\\\ \u0026= A \\times C \\\\ \u0026= \\{(1, \\alpha), (1, \\beta), (2, \\alpha), (2, \\beta), (3, \\alpha), (3, \\beta)\\} \\end{align*} \\]This is the exact same set we got in Step 1.\nIt’s worth noting that we could have side-stepped all of the above computation by using many of the previously discussed theorems like so:\n\\[ \\begin{align*} A \\times (B \\cup C) \u0026= A \\times (\\emptyset \\cup C) \\\\ \u0026= A \\times C \\\\ \u0026= \\emptyset \\cup (A \\times C) \\\\ \u0026= (A \\times \\emptyset) \\cup (A \\times C) \\\\ \u0026= (A \\times B) \\cup (A \\times C) \\end{align*} \\] Of course, we have a similar result for the intersection of sets:\nTheorem 5.3.3 For any arbitrary sets A, B, C, we have that\n\\[ \\begin{align*} A \\times (B \\cap C) \u0026= (A \\times B) \\cap (A \\times C) \\\\ (A \\cap B) \\times C \u0026= (A \\times C) \\cap (B \\times C) \\end{align*} \\] Proof 5.3.3 General Strategy: We’ll adopt a similar strategy we used in the proof of Theorem 5.3.2, except we’ll use logical equivalencies with $\\Longleftrightarrow$ instead of logical implications with $\\implies$. This will make the proof much more compact since we’ll only have to make one element argument instead of two.\nAgain, we’ll prove that\n$$A \\times (B \\cap C) = (A \\times B) \\cap (A \\times C)$$and omit the proof for\n$$(A \\cap B) \\times C = (A \\times C) \\cap (B \\times C)$$simply for the sake of brevity.\nWe begin by considering an arbitrary ordered pair (a, x) in the Cartesian Product $A \\times (B \\cap C)$:\n\\[ \\begin{array}{ r l l } \u0026 \\mathbf{(a, x) \\in A \\times (B \\cap C)} \u0026 \\textbf{Reason} \\\\ \\Longleftrightarrow \u0026 (a \\in A) \\land (x \\in B \\cap C) \u0026 \\text{Definition of Cartesian Product} \\\\ \\Longleftrightarrow \u0026 (a \\in A) \\land (x \\in B) \\land (x \\in C) \u0026 \\text{Definition of Intersection} \\\\ \\Longleftrightarrow \u0026 (a \\in A) \\land (a \\in A) \\land (x \\in B) \\land (x \\in C) \u0026 \\text{Idempotent Law of } \\land \\\\ \\Longleftrightarrow \u0026 (a \\in A) \\land (x \\in B) \\land (a \\in A) \\land (x \\in C) \u0026 \\text{Commutative Law of } \\land \\\\ \\Longleftrightarrow \u0026 [(a \\in A) \\land (x \\in B)] \\land [(a \\in A) \\land (x \\in C)] \u0026 \\text{Associative Law of } \\land \\\\ \\Longleftrightarrow \u0026 [(a, x) \\in A \\times B] \\land [(a, x) \\in A \\times C] \u0026 \\text{Definition of Cartesian Product} \\\\ \\Longleftrightarrow \u0026 (a, x) \\in (A \\times B) \\cap (A \\times C) \u0026 \\text{Definition of Intersection} \\end{array} \\]Thus, we have shown that\n$$(a, x) \\in A \\times (B \\cap C)\\ \\Longleftrightarrow\\ (a, x) \\in (A \\times B) \\cap (A \\times C)$$which is logically equivalent to showing both\n\\[ \\begin{align*} (a, x) \\in A \\times (B \\cap C) \u0026\\implies (a, x) \\in (A \\times B) \\cap (A \\times C) \\\\ (a, x) \\in (A \\times B) \\cap (A \\times C) \u0026\\implies (a, x) \\in A \\times (B \\cap C) \\end{align*} \\]meaning that we have that\n\\[ \\begin{align*} A \\times (B \\cap C) \u0026\\subseteq (A \\times B) \\cap (A \\times C) \\\\ (A \\times B) \\cap (A \\times C) \u0026\\subseteq A \\times (B \\cap C) \\end{align*} \\]This means that, by definition of set equality, we have\n$$A \\times (B \\cap C) = (A \\times B) \\cap (A \\times C)$$as desired!\nLet’s see some examples involving intersections.\nExample 5.3.4 Consider the sets\n\\[ \\begin{align*} A \u0026= \\{1, 2, 3\\} \\\\ B \u0026= \\{a, b\\} \\\\ C \u0026= \\{b, c\\} \\end{align*} \\]Step 1: Compute $A \\times (B \\cap C)$ Computing $B \\cap C$ yields\n\\[ \\begin{align*} B \\cap C \u0026= \\{a, b\\} \\cap \\{b, c\\} \\\\ \u0026= \\{b\\} \\end{align*} \\]Now we compute $A \\times (B \\cap C)$\n\\[ \\begin{align*} A \\times (B \\cap C) \u0026= \\{1, 2, 3\\} \\times \\{b\\} \\\\ \u0026= \\{(1, b), (2, b), (3, b)\\} \\end{align*} \\]Step 2: Compute $(A \\times B) \\cap (A \\times C)$ First, we compute $A \\times B$:\n\\[ \\begin{align*} A \\times B \u0026= \\{1, 2, 3\\} \\times \\{a, b\\} \\\\ \u0026= \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\end{align*} \\]Next, we compute $A \\times C$\n\\[ \\begin{align*} A \\times C \u0026= \\{1, 2, 3\\} \\times \\{b, c\\} \\\\ \u0026= \\{(1, b), (1, c), (2, b), (2, c), (3, b), (3, c)\\} \\end{align*} \\]Finally, we compute $(A \\times B) \\cap (A \\times C)$\n\\[ \\begin{align*} (A \\times B) \\cap (A \\times C) \u0026= \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\cap \\{(1, b), (1, c), (2, b), (2, c), (3, b), (3, c)\\} \\\\ \u0026= \\{(1, b), (2, b), (3, b)\\} \\end{align*} \\]This is the exact same result we got earlier in Step 1.\nExample 5.3.5 Consider the sets\n\\[ \\begin{align*} A \u0026= \\{1, 2, 3\\} \\\\ B \u0026= \\{a, b\\} \\\\ C \u0026= \\{\\alpha, \\beta\\} \\end{align*} \\]Because\n\\[ \\begin{align*} B \\cap C \u0026= \\{ a, b \\} \\cap \\{\\alpha, \\beta\\} \\\\ \u0026= \\emptyset \\end{align*} \\]we have by Theorem 5.3.1 that\n\\[ \\begin{align*} A \\times (B \\cap C) \u0026= A \\times \\emptyset \\\\ \u0026= \\emptyset \\end{align*} \\]Now when computing $(A \\times B) \\cap (A \\times C)$, we start by computing $A \\times B$ and $A \\times C$ like so:\n\\[ \\begin{align*} A \\times B \u0026= \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\\\ A \\times C \u0026= \\{(1, \\alpha), (1, \\beta), (2, \\alpha), (2, \\beta), (3, \\alpha), (3, \\beta)\\} \\end{align*} \\]$A \\times B$ and $A \\times C$ don’t have any common elements, meaning we have that\n$$(A \\times B) \\cap (A \\times C) = \\emptyset$$which is exactly what we got when computing $A \\times (B \\cap C)$.\nOnce again, we see that $A \\times (B \\cap C) = (A \\times B) \\cap (A \\times C)$.\nCartesian Products involving Subsets There is an interplay between Cartesian Products and subsets.\nTheorem 5.3.4 For any non-empty sets A, B, C, and D, we have that\n$$(A \\times B) \\subseteq (C \\times D) \\Longleftrightarrow (A \\subseteq C) \\land (B \\subseteq D)$$ Proof 5.3.4 General Strategy: Since we’re trying to prove a logical equivalency, we’ll break the proof up into two steps. In both steps, we’ll use element arguments by picking arbitrary ordered pairs of the form (x, y).\nStep 1: $(A \\times B) \\subseteq (C \\times D) \\implies (A \\subseteq C) \\land (B \\subseteq D)$ Suppose it were true that $(A \\times B) \\subseteq (C \\times D)$, and that A, B, C, and D were non-empty. Since A and B are non-empty, we must also have that $A \\times B$ is non-empty.\nSince $A \\times B$ is non-empty, we can pick out an arbitrary element from $A \\times B$, which we’ll refer to as (x, y).\nBecause $(x, y) \\in A \\times B$ and because $A \\times B \\subseteq C \\times D$, we must have that $(x, y) \\in C \\times D$. as well.\nNow, since $(x, y) \\in A \\times B$, we know (by the definition of Cartesian Product) that $(x \\in A) \\land (y \\in B)$.\nSince we also know that $(x, y) \\in C \\times D$, we similarly know that $(x \\in C) \\land (y \\land D)$.\nThus, whenever $x \\in A$, we also know that $x \\in C$, meaning that we have\n$$A \\subseteq C$$For similar reasons involving element y, we also have that\n$$B \\subseteq D$$Thus, we have established that\n$$(A \\times B) \\subseteq (C \\times D) \\implies (A \\subseteq C) \\land (B \\subseteq D)$$completing Step 1.\nStep 2: $(A \\subseteq C) \\land (B \\subseteq D) \\implies (A \\times B) \\subseteq (C \\times D)$ Suppose it were true that $A \\subseteq C$, $B \\subseteq D$ and that A, B, C, and D were all non-empty.\nBecause both A and B are non-empty, we have that $A \\times B$ is non-empty, meaning that we can pick an arbitrary element from $A \\times B$, which we’ll refer to as (x, y).\nBecause $(x, y) \\in A \\times B$, we have by definition of Cartesian Product that $x \\in A$ and $y \\in B$.\nNow, since $x \\in A$ and $A \\subseteq C$, we also have that $x \\in C$. Similarly, since $y \\in B$ and $B \\subseteq D$, we also have that $y \\in D$.\nSince $(x \\in C) \\land (y \\in D)$, we have by definition of Cartesian Product that $(x, y) \\in C \\times D$.\nThus, when ever $(x, y) \\in A \\times B$, we also have that $(x, y) \\in C \\times D$ as well. This means we have established that\n$$(A \\subseteq C) \\land (B \\subseteq D) \\implies (A \\times B) \\subseteq (C \\times D)$$completing Step 2.\nConclusion: $(A \\times B) \\subseteq (C \\times D) \\Longleftrightarrow (A \\subseteq C) \\land (B \\subseteq D)$ From Steps 1 and 2, we have established that\n\\[ \\begin{align*} (A \\times B) \\subseteq (C \\times D) \u0026\\implies (A \\subseteq C) \\land (B \\subseteq D) \\\\ (A \\subseteq C) \\land (B \\subseteq D) \u0026\\implies (A \\times B) \\subseteq (C \\times D) \\end{align*} \\]Therefore, we have that\n$$(A \\times B) \\subseteq (C \\times D) \\Longleftrightarrow (A \\subseteq C) \\land (B \\subseteq D)$$as desired.\nA couple of examples are in order.\nExample 5.3.6 Consider the sets\n\\[ \\begin{align*} A \u0026= \\{1, 2\\} \\\\ B \u0026= \\{a, b\\} \\\\ C \u0026= \\{1, 2, 3\\} \\\\ D \u0026= \\{a, b\\} \\\\ \\end{align*} \\]It’s easy to see that $A \\subseteq C$ and $B \\subseteq D$.\nLet’s compute both $A \\times B$ and $C \\times D$:\n\\[ \\begin{align*} A \\times B \u0026= \\{1, 2\\} \\times \\{a, b\\} \\\\ \u0026= \\{(1, a), (1, b), (2, a), (2, b)\\} \\\\ \\\\ C \\times D \u0026= \\{1, 2, 3\\} \\times \\{a, b\\} \\\\ \u0026= \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\end{align*} \\]It’s easy to see that all four ordered pairs in $A \\times B$ are in $C \\times D$:\n\\[ \\begin{align*} (1, a) \u0026\\in \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\\\ (1, b) \u0026\\in \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\\\ (2, a) \u0026\\in \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\\\ (2, b) \u0026\\in \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\\\ \\end{align*} \\]Thus we have that $(A \\times B) \\subseteq (C \\times D)$ as expected.\nExample 5.3.7 Consider the sets\n\\[ \\begin{align*} A \u0026= \\{1, 2, 3\\} \\\\ B \u0026= \\{a, b\\} \\\\ C \u0026= \\{8, 9\\} \\\\ D \u0026= \\{x, y, z\\} \\\\ \\end{align*} \\]Notice that A is not a subset of C, and B is not a subset of D. Thus we’d expect that since\n$$(A \\subseteq C) \\land (B \\subseteq D) = 0$$and\n$$(A \\times B) \\subset (C \\times D) \\Longleftrightarrow (A \\subseteq C) \\land (B \\subseteq D)$$we’d have by Theorem 5.3.4 that\n$$(A \\times B) \\subseteq (C \\times D) = 0$$meaning $A \\times B$ is not a subset of $C \\times D$ (expressed in the language of propositions).\nWe can avoid computing both $A \\times B$ and $C \\times D$ by noting that since $1 \\in A$ the set $A \\times B$ will have ordered pairs where the first element is 1. However, since $1 \\notin C$, none of the ordered pairs in $C \\times D$ will have a 1 as the first element. This means that $A \\times B$ has at least one element not in $C \\times D$, and so we have that\n$$A \\times B \\nsubseteq C \\times D$$as expected.\nWe could compute $A \\times B$ and $C \\times D$, however by stopping and thinking, we saved ourselves a lot of work!\nNotice that Theorem 4.2.4 required that A, B, C, and D all be non-empty. Indeed, the proof of Theorem 4.2.4 required that we be able to pick arbitrary elements from A and B, as well as showing that they were also in C and D respectively.\nWhat happens when one or more of the sets involved is the Empty Set?\nExample 5.3.8 Consider the sets\n\\[ \\begin{align*} A \u0026= \\emptyset \\\\ B \u0026= \\{a, b\\} \\\\ C \u0026= \\{1, 2, 3\\} \\\\ D \u0026= \\{a, b\\} \\end{align*} \\]Here, we see that both $A \\subseteq C$ and $B \\subseteq D$.\nFurthermore, we see that\n\\[ \\begin{align*} A \\times B \u0026= \\emptyset \\times \\{a, b\\} \\\\ \u0026= \\emptyset \\\\ \\\\ C \\times D \u0026= \\{1, 2, 3\\} \\times \\{a, b\\} \\\\ \u0026= \\{(1, a), (1, b), (2, a), (2, b), (3, a), (3, b)\\} \\end{align*} \\]and so we can easily see that $(A \\times B) \\subseteq (C \\times D)$.\nUsing the language of propositional logic, we would say that the proposition $(A \\subseteq C) \\land (B \\subseteq D)$ evaluates to true; in other words, we have that\n$$(A \\subseteq C) \\land (B \\subseteq D) = 1$$Furthermore we see that the proposition $(A \\times B) \\subseteq (C \\times D)$ also evaluates to true, meaning we have that\n$$(A \\times B) \\subseteq (C \\times D) = 1$$As such, we see that\n\\[ \\begin{align*} (A \\times B) \\subseteq (C \\times D) \\longleftrightarrow (A \\subseteq C) \\land (B \\subseteq D) \u0026= 1 \\longleftrightarrow 1 \\\\ \u0026= 1 \\end{align*} \\]We see that introducing the Empty Set does not necessarily “break” Theorem 5.3.4 because it held true in this case. But is that always the case? What if we make a different set empty?\nExample 5.3.9 Consider the sets\n\\[ \\begin{align*} A \u0026= \\{1, 2\\} \\\\ B \u0026= \\{a, b\\} \\\\ C \u0026= \\{1, 2, 3\\} \\\\ D \u0026= \\emptyset \\end{align*} \\]Here, we see that while $A \\subseteq C$, we have that $B \\nsubseteq D$, meaning\n$$(A \\subseteq C) \\land (B \\subseteq D) = 0$$Computing $A \\times B$ and $C \\times D$ yields the following:\n\\[ \\begin{align*} A \\times B \u0026= \\{1, 2\\} \\times \\{a, b\\} \\\\ \u0026= \\{(1, a), (1, b), (2, a), (2, b)\\} \\\\ \\\\ C \\times D \u0026= \\{1, 2, 3\\} \\times \\emptyset \\\\ \u0026= \\emptyset \\end{align*} \\]Thus we see that $(A \\times B) \\nsubseteq (C \\times D)$, meaning\n$$(A \\times B) \\subseteq (C \\times D) = 0$$Since both propositions $(A \\subseteq C) \\land (B \\subseteq D) = 0$ and $(A \\times B) \\subseteq (C \\times D) = 0$ evaluated to false, Theorem 5.3.4 seems to still hold true in this case as well. Again, we can use propositional logic to more closely examine the situation:\n\\[ \\begin{align*} (A \\times B) \\subseteq (C \\times D) \\longleftrightarrow (A \\subseteq C) \\land (B \\subseteq D) \u0026= 0 \\longleftrightarrow 0 \\\\ \u0026= 1 \\end{align*} \\] In Example 5.3.8 and Example 5.3.9, we saw that only making one of four sets empty, the proposition\n$$(A \\times B) \\subseteq (C \\times D) \\longleftrightarrow (A \\subseteq C) \\land (B \\subseteq D)$$still evaluated to true. Let’s examine one more example where more than one set is empty.\nExample 5.3.10 Consider the sets\n\\[ \\begin{align*} A \u0026= \\{1, 2\\} \\\\ B \u0026= \\emptyset \\\\ C \u0026= \\emptyset \\\\ D \u0026= \\{a, b\\} \\end{align*} \\]We can immediately see that $A \\nsubseteq C$ and that $B \\subseteq D$.\nComputing $A \\times B$ and $C \\times D$ is easy since $B = D = \\emptyset$:\n\\[ \\begin{align*} A \\times B \u0026= \\{1, 2\\} \\times \\emptyset \\\\ \u0026= \\emptyset \\\\ \\\\ C \\times D \u0026= \\emptyset \\times \\{a, b\\} \\\\ \u0026= \\emptyset \\end{align*} \\]Of course we have that $\\emptyset \\subseteq \\emptyset$.\nAgain, we can examine this more closely using propositional logic:\n\\[ \\begin{align*} (A \\times B) \\land (C \\times D) \\longleftrightarrow (A \\subseteq C) \\land (B \\subseteq D) \u0026= 1 \\longleftrightarrow 0 \\\\ \u0026= 0 \\end{align*} \\]So, having two empty sets is enough to make the proposition false, meaning we finally “broke” Theorem 5.3.4.\nTheorem 5.3.4 requires all involved sets need to be non-empty. Even if some of the sets are empty, both $(A \\times B) \\land (C \\times D)$ and $(A \\subseteq C) \\land (B \\subseteq D)$ can evaluate to the same logical value, but Theorem 5.3.4 does not apply in those situations. Instead, other methods must be employed, such as verifying by hand.\nExperiment with Theorem Premises Always pay careful attention to the premises of any theorem you are working with. If even one premise is not met, then the theorem does not apply. It’s still possible that the conclusion of the theorem could be true, but not because of the theorem itself. Instead, other theorems, definitions, or axioms must be used to establish the conclusion’s truth.\nIt’s good practice to experiment and come up with scenarios where not all of the theorem’s premises are true. Trying to “break” a theorem is a great way to understand why a theorem is true.\n$\\times$ Distributes Over - and $\\triangle$ There are two more operators to discuss. First, we’ll look at how $\\times$ interacts with set differences.\nTheorem 5.3.5 For any sets A, B, and C, we have that\n$$A \\times (B - C) = (A \\times B) - (A \\times C)$$ Proof 5.3.5 General Strategy: The proof is relatively straight-forward, relying mainly on the definitions of various set operators. The actual proof will be an element argument.\nLet (a, x) be an arbitrarily picked element from $A \\times (B - C)$.\n\\[ \\begin{array}{ r l l } \u0026 \\mathbf{(a, x) \\in A \\times (B - C)} \u0026 \\textbf{Reason} \\\\ \\Longleftrightarrow \u0026 (a \\in A) \\land (x \\in B - C) \u0026 \\text{Definition of Cartesian Product} \\\\ \\Longleftrightarrow \u0026 (a \\in A) \\land (x \\in B) \\land (x \\notin C) \u0026 \\text{Definition of Set Difference} \\\\ \\Longleftrightarrow \u0026 (a \\in A) \\land (a \\in A) \\land (x \\in B) \\land (x \\notin C) \u0026 \\text{Idempotent Law of } \\land \\\\ \\Longleftrightarrow \u0026 (a \\in A) \\land (x \\in B) \\land (a \\in A) \\land (x \\notin C) \u0026 \\text{Commutative Law of } \\land \\\\ \\Longleftrightarrow \u0026 \\bigl[(a \\in A) \\land (x \\in B)\\bigr] \\land \\bigl[(a \\in A) \\land (x \\notin C)\\bigr] \u0026 \\text{Associative Law of } \\land \\\\ \\Longleftrightarrow \u0026 (a, x) \\in (A \\times B) \\land (a, x) \\notin (A \\times C) \u0026 \\text{Definition of Cartesian Product} \\\\ \\Longleftrightarrow \u0026 (a, x) \\in (A \\times B) - (A \\times C) \u0026 \\text{Definition of Set Difference} \\end{array} \\]Showing that $(a, x) \\in A \\times (B - C) \\Longleftrightarrow (a, x) \\in (A \\times B) - (A \\times C)$ is of course equivalent to showing that\n\\[ \\begin{align*} (a, x) \\in A \\times (B - C) \u0026\\implies (a, x) \\in (A \\times B) - (A \\times C) \\\\ (a, x) \\in (A \\times B) - (A \\times C) \u0026\\implies (a, x) \\in A \\times (B - C) \\end{align*} \\]which (by definition of subset) means we have also shown that\n\\[ \\begin{align*} A \\times (B - C) \u0026\\subseteq (A \\times B) - (A \\times C) \\\\ (A \\times B) - (A \\times C) \u0026\\subseteq A \\times (B - C) \\end{align*} \\]This means that we have shown that\n$$A \\times (B - C) = (A \\times B) - (A \\times C)$$If we can’t pick an arbitrary element from $A \\times (B - C)$, then $A \\times (B - C)$ would have to be empty, meaning either $A = \\emptyset$ or $B - C = \\emptyset$.\nIf $A = \\emptyset$, then we would have that\n\\[ \\begin{align*} (A \\times B) - (A \\times C) \u0026= (\\emptyset \\times B) - (\\emptyset \\times C) \\\\ \u0026= \\emptyset - \\emptyset \\\\ \u0026= \\emptyset \\\\ \u0026= \\emptyset \\times (B - C) \\\\ \u0026= A \\times (B - C) \\end{align*} \\]If $B - C = \\emptyset$, then we would have that $B \\subseteq C$. Furthermore, since $A \\subseteq A$, Theorem 5.3.4 tells us that $(A \\times B) \\subseteq (A \\times C)$, meaning we have that\n\\[ \\begin{align*} (A \\times B) - (A \\times C) \u0026= \\emptyset \\\\ \u0026= A \\times \\emptyset \\\\ \u0026= A \\times (B - C) \\end{align*} \\]Whether $A \\times (B - C)$ is empty or not, we see that we always get that\n$$A \\times (B - C) = (A \\times B) - (A \\times C)$$as desired.\nWe can use Theorem 5.3.5 to help us prove the next theorem.\nTheorem 5.3.6 For any sets A, B, C, we have that\n$$A \\times (B\\ \\triangle\\ C) = (A \\times B)\\ \\triangle\\ (A \\times C)$$ Proof 5.3.6 General Strategy: We’ll use a bunch of previous theorems and definitions to directly establish equality. This will allow us to avoid having to use an element argument.\n\\[ \\begin{array}{ r l l } \u0026 A \\times (B\\ \\triangle\\ C) \u0026 \\textbf{Reason} \\\\ = \u0026 A \\times \\bigl[ (B - C) \\cup (C - B) \\bigr] \u0026 \\text{Definition of Symmetric Difference} \\\\ = \u0026 \\bigl[ A \\times (B - C) \\bigr] \\cup \\bigl[ A \\times (C - B) \\bigr] \u0026 \\text{Theorem 5.3.2} \\\\ = \u0026 \\bigl[ (A \\times B) - (A \\times C) \\bigr] \\cup \\bigl[ (A \\times C) - (A \\times B) \\bigr] \u0026 \\text{Theorem 5.3.5} \\\\ = \u0026 (A \\times B)\\ \\triangle\\ (A \\times C) \u0026 \\text{Definition of Symmetric Difference} \\end{array} \\]Thus, we have just shown that\n$$A \\times (B\\ \\triangle\\ C) = (A \\times B)\\ \\triangle\\ (A \\times C)$$as desired.\nLet’s see some examples.\nExample 5.3.11 Consider the sets\n\\[ \\begin{align*} A \u0026= \\{x, y, z\\} \\\\ B \u0026= \\{1, 2, 3\\} \\\\ C \u0026= \\{1, 4\\} \\end{align*} \\]According to Theorem 5.3.5, we’d expect to see that $A \\times (B - C) = (A \\times B) - (A \\times C)$, so let’s verify that is the case.\nFirst, we compute $A \\times (B - C)$:\n\\[ \\begin{align*} A \\times (B - C) \u0026= \\{x, y, z\\} \\times \\bigl(\\{1, 2, 3\\} - \\{1, 4\\}\\bigr) \\\\ \u0026= \\{x, y, z\\} \\times \\{2, 3\\} \\\\ \u0026= \\{(x, 2), (x, 3), (y, 2), (y, 3), (z, 2), (z, 3)\\} \\end{align*} \\]Next, we compute $(A \\times B) - (A \\times C)$. First, we compute $A \\times B$. Next we compute $A \\times C$.\n\\[ \\begin{align*} A \\times B \u0026= \\{x, y, z\\} \\times \\{1, 2, 3\\} \\\\ \u0026= \\{(x, 1), (x, 2), (x, 3), (y, 1), (y, 2), (y, 3), (z, 1), (z, 2), (z, 3)\\} \\\\ \\\\ A \\times C \u0026= \\{x, y, z\\} \\times \\{1, 4\\} \\\\ \u0026= \\{(x, 1), (x, 4), (y, 1), (y, 4), (z, 1), (z, 4)\\} \\end{align*} \\]We can see that since $A \\times B$ and $A \\times C$ both contain the elements (x, 1), (y, 1), and (z, 1), those ordered pairs will not appear in the set difference:\n\\[ \\begin{align*} (A \\times B) - (A \\times C) \u0026= (A \\times B) \\\\ \u0026= \\{(x, 2), (x, 3), (y, 2), (y, 3), (z, 2), (z, 3)\\} \\\\ \u0026= A \\times (B - C) \\end{align*} \\]Thus we see that Theorem 5.3.5 holds.\nExample 5.3.12 Reconsider the sets from Example 5.3.11:\n\\[ \\begin{align*} A \u0026= \\{x, y, z\\} \\\\ B \u0026= \\{1, 2, 3\\} \\\\ C \u0026= \\{1, 4\\} \\end{align*} \\]Theorem 5.3.6 says that the two sets $A \\times (B\\ \\triangle\\ C)$ and $(A \\times B)\\ \\triangle\\ (A \\times C)$ are equal. Let’s verify that is the case by first computing $A \\times (B\\ \\triangle\\ C)$:\n\\[ \\begin{align*} A \\times (B\\ \\triangle\\ C) \u0026= \\{x, y, z\\} \\times \\bigl(\\{1, 2, 3\\}\\ \\triangle\\ \\{1, 4\\}\\bigr) \\\\ \u0026= \\{x, y, z\\} \\times \\{2, 3, 4\\} \\\\ \u0026= \\{(x, 2), (x, 3), (x, 4), (y, 2), (y, 3), (y, 4), (z, 2), (z, 3), (z, 4)\\} \\end{align*} \\]Now we compute $(A \\times B)\\ \\triangle\\ (A \\times C)$:\n\\[ \\begin{align*} A \\times B \u0026= \\{x, y, z\\} \\times \\{1, 2, 3\\} \\\\ \u0026= \\{(x, 1), (x, 2), (x, 3), (y, 1), (y, 2), (y, 3), (z, 1), (z, 2), (z, 3)\\} \\\\ \\\\ A \\times C \u0026= \\{x, y, z\\} \\times \\{1, 4\\} \\\\ \u0026= \\{(x, 1), (x, 4), (y, 1), (y, 4), (z, 1), (z, 4)\\} \\\\ \\\\ (A \\times B)\\ \\triangle\\ (A \\times C) \u0026= \\{(x, 2), (x, 3), (x, 4), (y, 2), (y, 3), (y, 4), (z, 2), (z, 3), (z, 4)\\} \\\\ \u0026= A \\times (B\\ \\triangle\\ C) \\end{align*} \\]And so it appears that Theorem 5.3.6 has come through. Of course, it looks like if we ever need to compute $(A \\times B)\\ \\triangle\\ (A \\times C)$ for whatever reason, we can save a lot of work by computing $A \\times (B\\ \\triangle\\ C)$ instead.",
    "description": "The Cartesian Product of two sets is just another set. This means we can also use other set operations like union, intersection, and set differences.\nIn this section, we explore how the Cartesian Product interacts with the other set operators.\nAssociativity of the Cartesian Product We already discussed how order matters in Cartesian Products, and as such sets don’t commute around the Cartesian Product operator $\\times$. Associativity is another common property we should examine. What exactly is the difference between",
    "tags": [],
    "title": "Properties of the Cartesian Product",
    "uri": "/the-book-of-foundational-mathematics/relations/properties-of-the-cartesian-product/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics",
    "content": "This is a new chapter.",
    "description": "This is a new chapter.",
    "tags": [],
    "title": "Ubiquitous Sets of Numbers",
    "uri": "/the-book-of-foundational-mathematics/ubiquitous-sets-of-numbers/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Relations",
    "content": "When we form the Cartesian Product $A \\times B$, we are essentially pairing up every element in A with every element in B. However, we may not want to associate every element of A with every element in B. Instead, we may only want to pair up certain elements together because they satisfy some condition.\nThe way we pair up only certain elements from A to certain elements of B is to take a subset of $A \\times B$. These kinds of subsets are yet another fundamental structure that appear all throughout all of mathematics.\nDefining Relations As stated above, we often want to take subsets of Cartesian Products.\nRelation A subset of $A \\times B$ is called a relation; more specifically, it is called a (binary) relation from A to B. Typically, a relation is denoted by the symbol $\\mathcal{R}$.\nFor a relation $\\mathcal{R}$ derived from $A \\times B$, whenever we have that\n$$(a, b) \\in \\mathcal{R}$$we can instead write\n$$a\\ \\mathcal{R}\\ b$$to specify that element a is related to element b.\nThe reason the word binary was used in the definition is because there were two sets being used to form the relation. Let’s look at a few examples.\nExample 5.4.1 Consider the following two sets:\n\\[ \\begin{align*} A \u0026= \\{1, 2, 3, 4, 5\\} \\\\ B \u0026= \\{2, 4, 6, 8, 10\\} \\end{align*} \\]We have that\n\\[ \\begin{array}{c c c c c c c c c} A \\times B \u0026 = \u0026 \\{ \u0026 (1, 2), \u0026 (1, 4), \u0026 (1, 6), \u0026 (1, 8), \u0026 (1, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (2, 2), \u0026 (2, 4), \u0026 (2, 6), \u0026 (2, 8), \u0026 (2, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (3, 2), \u0026 (3, 4), \u0026 (3, 6), \u0026 (3, 8), \u0026 (3, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (4, 2), \u0026 (4, 4), \u0026 (4, 6), \u0026 (4, 8), \u0026 (4, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (5, 2), \u0026 (5, 4), \u0026 (5, 6), \u0026 (5, 8), \u0026 (5, 10) \u0026 \\} \\\\ \\end{array} \\]where we spaced out the elements to make it easier to see what elements are in $A \\times B$.\nFor our first example of a relation, let’s just pick a subset of $A \\times B$ consisting of arbitrarily picked elements without any rhyme or reason:\n$$\\mathcal{R}_1 = \\{(1, 2), (3, 2), (2, 4), (3, 4), (1, 6), (1, 8), (2, 8), (5, 8), (5, 10)\\}$$For this first relation, we see that the number 1, taken from set A, is related to a few different numbers from B:\n\\[ \\begin{align*} 1\\ \u0026\\mathcal{R}_1\\ 2 \\\\ 1\\ \u0026\\mathcal{R}_1\\ 6 \\\\ 1\\ \u0026\\mathcal{R}_1\\ 8 \\end{align*} \\]We also see that 3 (taken from set A) is also related to a couple of elements from B:\n\\[ \\begin{align*} 3\\ \\mathcal{R}_1\\ 2 \\\\ 3\\ \\mathcal{R}_1\\ 4 \\end{align*} \\]We also see that numbers 2 and 5 from A are also related to a few different numbers from B:\n\\[ \\begin{array}{ c c } 2\\ \\mathcal{R}_1\\ 4 \u0026 2\\ \\mathcal{R}_1\\ 8 \\\\ 5\\ \\mathcal{R}_1\\ 8 \u0026 5\\ \\mathcal{R}_1\\ 10 \\end{array} \\] Example 5.4.2 Reconsider the Cartesian Product\n\\[ \\begin{array}{c c c c c c c c c} A \\times B \u0026 = \u0026 \\{ \u0026 (1, 2), \u0026 (1, 4), \u0026 (1, 6), \u0026 (1, 8), \u0026 (1, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (2, 2), \u0026 (2, 4), \u0026 (2, 6), \u0026 (2, 8), \u0026 (2, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (3, 2), \u0026 (3, 4), \u0026 (3, 6), \u0026 (3, 8), \u0026 (3, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (4, 2), \u0026 (4, 4), \u0026 (4, 6), \u0026 (4, 8), \u0026 (4, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (5, 2), \u0026 (5, 4), \u0026 (5, 6), \u0026 (5, 8), \u0026 (5, 10) \u0026 \\} \\\\ \\end{array} \\]from Example 5.4.1.\nInstead of picking arbitrary elements, let’s choose elements that satisfy some\nrule.\nOne relation we could take is\n$$(a, b) \\in\\ \\mathcal{R}_2\\ \\text{ if } b = 2a$$One such ordered pair is (2, 4) because 4 = 2(2). Another such ordered pair is (1, 2) because 2 = 2(1). There are other ordered pairs in the relation:\n$$\\mathcal{R}_2 = \\{(1, 2), (2, 4), (3, 6), (4, 8), (5, 10)\\}$$Another relation we could take is\n$$\\mathcal{R}_3 = \\{(a, b)\\ |\\ a = 3 \\lor b = 2\\}$$where the ordered pairs we are interested in either have 3 as the first element, 2 as the second element, or both. This relation is the set\n$$\\mathcal{R}_3 = \\{(3, 2), (3, 4), (3, 6), (3, 8), (3, 10), (1, 2), (2, 2), (4, 2), (5, 2)\\}$$Going off $\\mathcal{R}_3$, we can also use the logical and operation:\n\\[ \\begin{align*} \\mathcal{R}_4 \u0026= \\{(a, b)\\ |\\ a = 3 \\land b = 2\\} \\\\ \u0026= \\{(3, 2)\\} \\end{align*} \\] Example 5.4.3 Reconsider the Cartesian Product from Example 5.4.1 again:\n\\[ \\begin{array}{c c c c c c c c c} A \\times B \u0026 = \u0026 \\{ \u0026 (1, 2), \u0026 (1, 4), \u0026 (1, 6), \u0026 (1, 8), \u0026 (1, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (2, 2), \u0026 (2, 4), \u0026 (2, 6), \u0026 (2, 8), \u0026 (2, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (3, 2), \u0026 (3, 4), \u0026 (3, 6), \u0026 (3, 8), \u0026 (3, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (4, 2), \u0026 (4, 4), \u0026 (4, 6), \u0026 (4, 8), \u0026 (4, 10), \u0026 \\\\ \u0026 \u0026 \u0026 (5, 2), \u0026 (5, 4), \u0026 (5, 6), \u0026 (5, 8), \u0026 (5, 10) \u0026 \\} \\\\ \\end{array} \\]Now consider the following relation:\n$$\\mathcal{R}_5 = \\{(a, b)\\ |\\ a = 3 \\land a = 5\\}$$Notice that none of the ordered pairs in $A \\times B$ have a first element that is simultaneously equal to 3 and 5. As such, we have that\n$$\\mathcal{R}_5 = \\emptyset$$meaning that $\\mathcal{R}_5$ is an empty relation.\nRelations on a Single Set Just like we can take the Cartesian Product of a set with itself such as $A \\times A$, we can also take subsets of $A \\times A$ to form relations from A to itself. In this case, we would call $\\mathcal{R}$ a binary relation on A. Again, we use the word binary to describe the fact that there are two sets being used to form the Cartesian Product.\nExample 5.4.4 Consider the set $A = \\{-2, -1, 0, 1, 2\\}$.\nThe Cartesian Product $A^2$ is thus\n\\[ \\begin{array}{ c c c c c c c c c } A \\times A \u0026 = \u0026 \\{ \u0026 (-2, -2), \u0026 (-2, -1), \u0026 (-2, 0), \u0026 (-2, 1), \u0026 (-2, 2), \u0026 \\\\ \u0026 \u0026 \u0026 (-1, -2), \u0026 (-1, -1), \u0026 (-1, 0), \u0026 (-1, 1), \u0026 (-1, 2), \u0026 \\\\ \u0026 \u0026 \u0026 (0, -2), \u0026 (0, -1), \u0026 (0, 0), \u0026 (0, 1), \u0026 (0, 2), \u0026 \\\\ \u0026 \u0026 \u0026 (1, -2), \u0026 (1, -1), \u0026 (1, 0), \u0026 (1, 1), \u0026 (1, 2), \u0026 \\\\ \u0026 \u0026 \u0026 (2, -2), \u0026 (2, -1), \u0026 (2, 0), \u0026 (2, 1), \u0026 (2, 2) \u0026 \\} \\end{array} \\]Consider the relation\n$$(a_1, a_2) \\in \\mathcal{R} \\text{ if } a_1 \u003c a_2$$This is the “less than” relation, so instead of writing $\\mathcal{R}$, we could use the symbol \u003c instead. As such, instead of writing $-1\\ \\mathcal{R}\\ 0$, we can now write $-1 \u003c 0$, which coincides with the commonly used mathematical notation. Here, we see that\n$$\\mathcal{R}\\ =\\ \u003c\\ =\\ \\{(-2, -1), (-2, 0), (-2, 1), (-2, 2), (-1, 0), (-1, 1), (-1, 2), (0, 1), (0, 2), (1, 2)\\}$$Note that we are using the symbol \u003c both as a comparison operator (its common usage), and as the name for a set (which is very unusual, and tends to be avoided to prevent confusion).\nExample 5.4.5 Consider the set\n$$\\bigstar = \\{0, 0.3, 0.7, 1, 1.5, 1.7, 2, 2.1, 2.11\\}$$Let $\\mathcal{R}$ be the binary relation on $\\bigstar$ defined by\n$$(a, b) \\in \\mathcal{R} \\text{ if } b = \\lceil a \\rceil$$where the second element is the ceiling of the first element. For example, because $\\lceil 0.3 \\rceil = 1$ and $\\lceil 2 \\rceil = 2$, we have that\n\\[ \\begin{align*} 0.3\\ \u0026\\mathcal{R}\\ 1 \\\\ 2\\ \u0026\\mathcal{R}\\ 2 \\end{align*} \\]We thus have that\n\\[ \\begin{align*} \\mathcal{R} \u0026= \\{\\ (x, y) \\in \\bigstar \\times \\bigstar \\ | \\ y = \\lceil x \\rceil\\} \\\\ \u0026= \\{\\ (0, \\lceil 0 \\rceil), (0.3, \\lceil 0.3 \\rceil), (0.7, \\lceil 0.7 \\rceil), (1, \\lceil 1 \\rceil), (1.5, \\lceil 1.5 \\rceil), (1.7, \\lceil 1.7 \\rceil), (2, \\lceil 2 \\rceil) \\ \\} \\\\ \u0026= \\{\\ (0, 0), (0.3, 1), (0.7, 1), (1, 1), (1.5, 2), (1.7, 2), (2, 2) \\ \\} \\end{align*} \\]Notice that since $3 \\notin \\bigstar$, we have that\n\\[ \\begin{align*} 2.1 \u0026\\cancel{\\mathcal{R}}\\ 3 \\\\ 2.11 \u0026\\cancel{\\mathcal{R}}\\ 3 \\end{align*} \\]even though the ceiling of both 2.1 and 2.11 is 3. This is because 3 is not in $\\bigstar$, meaning we have that\n\\[ \\begin{align*} (2.1, 3) \u0026\\notin \\bigstar \\times \\bigstar \\\\ (2.11, 3) \u0026\\notin \\bigstar \\times \\bigstar \\end{align*} \\]which further means that neither (2.1, 3) nor (2.11, 3) can possibly be in any subset of $\\bigstar \\times \\bigstar$.\nRelations on Three or More Sets Of course, there is nothing stopping us from considering subsets of Cartesian Products of three or more sets.\nExample 5.4.6 Consider the set of primary pigment colors and secondary pigment colors: $~$ \\[ \\begin{align*} P \u0026= \\{\\text{Red}, \\text{Yellow}, \\text{Blue}\\} \\\\ S \u0026= \\{\\text{Orange}, \\text{Green}, \\text{Purple}\\} \\end{align*} \\] Figure 5.4.1: This diagram shows how the primary pigment colors mix together to form the secondary pigment colors.\nWe can form a relation on the Cartesian Product $P \\times P \\times S$ by considering triples where the first two elements mix together to get a secondary color according to the diagram above showing how pigment colors mix together: $$\\mathcal{R} = \\{(\\text{Red}, \\text{Yellow}, \\text{Orange}), (\\text{Red}, \\text{Blue}, \\text{Purple}), (\\text{Yellow}, \\text{Blue}, \\text{Green})\\}$$ Example 5.4.7 Alternatively to pigments, light can mix together to form new colors as well, as demonstrated in Figure 5.4.2 below. Figure 5.4.2: Different wave lengths of light are different colors, which can be mixed together to form new colors. The primary colors here are Red, Green, and Blue. Pixels on screens are colored by emitting different combinations of those colors.\nConsider the set P of primary light colors and the set S of secondary light colors: \\[ \\begin{align*} P \u0026= \\{\\text{Red}, \\text{Green}, \\text{Blue}\\} \\\\ S \u0026= \\{\\text{Yellow}, \\text{Magenta}, \\text{Cyan}\\} \\end{align*} \\] Differently to Example 5.4.6, let's take a relation on the Cartesian Product $(P \\times P) \\times S$, which should make it easier to see what colors are mixing together to form the third color. The relation we'll take is the relation of color mixtures as defined in Figure 5.4.2 above: $$\\mathcal{R} = \\{((\\text{Red}, \\text{Green}), \\text{Yellow}), ((\\text{Red}, \\text{Blue}), \\text{Magenta}), ((\\text{Green}, \\text{Blue}), \\text{Cyan})\\}$$ Example 5.4.8 Astrid is a graphic artist for a large online magazine, and wants to upgrade her computer. Her local computer shop sells computers with four categories of specifications for the CPU, Graphics card, RAM, and Motherboard based on the following:\nThe number of CPU cores. The amount of video RAM (VRAM), measured in Gigabytes, on the Graphics card. The amount of RAM available, measured in Gigabytes, to the CPU. Whether or not the Motherboard has on-board wifi. The store offers the following options for each category:\n\\[ \\begin{align*} \\text{CPU} \u0026= \\{4, 8, 12, 16\\} \\\\ \\text{Graphics Card} \u0026= \\{8, 16, 24\\} \\\\ \\text{RAM} \u0026= \\{16, 32\\} \\\\ \\text{Wifi} \u0026= \\{\\text{Yes}, \\text{No}\\} \\end{align*} \\]Thus, all of the available configurations the store sells are captured in the Cartesian Product\n$$\\text{CPU} \\times \\text{Graphics Card} \\times \\text{RAM} \\times \\text{Wifi}$$Astrid needs a computer that can handle digital art and 3D rendering, so she wants a computer with at least 16Gb VRAM and 32Gb RAM. However, she also wants to try and save money by not splurging on a CPU, so she wants no more than 8 CPU cores. She doesn’t care if the motherboard has on-board wifi or not.\nThus, we can form a relation $\\mathcal{R}_{\\text{Astrid}}$ on $\\text{CPU} \\times \\text{Graphics Card} \\times \\text{RAM} \\times \\text{Wifi}$ by considering all options that meet Astrid’s requirement:\n\\[ \\begin{align*} \\mathcal{R}_{\\text{Astrid}} \u0026= \\{(c, v, r, w) \\in \\text{CPU} \\times \\text{Graphics Card} \\times \\text{RAM} \\times \\text{Wifi}\\ |\\ c \\le 8 \\land v \\ge 16 \\land r \\ge 32\\} \\\\ \\\\ \u0026= \\begin{array}{ c c c c c c } \\{ \u0026 (4, 16, 32, \\text{No}), \u0026 (4, 24, 32, \\text{No}), \u0026 (8, 16, 32, \\text{No}), \u0026 (8, 24, 32, \\text{No}), \u0026 \\\\ \u0026 (4, 16, 32, \\text{Yes}), \u0026 (4, 24, 32, \\text{Yes}), \u0026 (8, 16, 32, \\text{Yes}), \u0026 (8, 24, 32, \\text{Yes}) \u0026 \\} \\end{array} \\end{align*} \\]",
    "description": "When we form the Cartesian Product $A \\times B$, we are essentially pairing up every element in A with every element in B. However, we may not want to associate every element of A with every element in B. Instead, we may only want to pair up certain elements together because they satisfy some condition.\nThe way we pair up only certain elements from A to certain elements of B is to take a subset of $A \\times B$. These kinds of subsets are yet another fundamental structure that appear all throughout all of mathematics.",
    "tags": [],
    "title": "Relations",
    "uri": "/the-book-of-foundational-mathematics/relations/relations/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics",
    "content": "Relations are used all throughout mathematics. Examples of relations abound in subjects such as algebra, trigonometry, calculus, as well as computer science, physics, chemistry, biology, and economics.",
    "description": "Relations are used all throughout mathematics. Examples of relations abound in subjects such as algebra, trigonometry, calculus, as well as computer science, physics, chemistry, biology, and economics.",
    "tags": [],
    "title": "Relations",
    "uri": "/the-book-of-foundational-mathematics/relations/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Relations",
    "content": "As seen in the previous section, some relations involve nested structures. For example, some relations have elements that contain ordered pairs. Remember that all elements in a relation are ordered pairs themselves, meaning the elements of some relations are ordered pairs that also contain ordered pairs. As such, when we were listing out all of the elements of a relation, we sometimes spaced them out to make it easier to see what was in each of the elements. We saw this in Example 4.4.1, and Example 4.4.8.\nThere are ways we can visually represent the contents of Cartesian Products and relations defined on those Cartesian Products. Even though diagrams may not provide rigorous logic that can’t always be used to establish results, they can help us reason about issues, and can also provide some clarity.\nHere, we’ll discuss three common ways to visualize Cartesian Products and relations. The first method we discuss will be especially helpful in the next section where we describe binary relations on a single set.\n0-1 Tables For some Cartesian Product $A \\times B$, and relation $\\mathcal{R} \\subseteq A \\times B$, remember that $(a, b) \\in \\mathcal{R}$ and $a\\ \\mathcal{R}\\ b$ are propositions (the exact same proposition). This means that they have a truth value of either true or false. This means that we can tabulate which elements in A and B are related by simply evaluating the propositions.\nTo help facilitate this process, we construct a table similar to the kind of table we used to describe Cartesian Products. Except in this new table, we’ll write the value of the proposition.\nIn other words, to write the table for relation $\\mathcal{R}$ defined on the Cartesian Product $A \\times B$, we consider the $a_i$ row and the $b_j$ column. Instead of writing the ordered pair $(a_i, b_j)$ in the $a_i^{\\text{th}}$ row and the $b_j^{\\text{th}}$ column, we write whatever $a_i\\ \\mathcal{R}\\ b_j$ evaluates to.\n$\\mathcal{R}$ $A / B$ $b_1$ $b_2$ $\\cdots$ $b_m$ $a_1$ $a_1\\ \\mathcal{R}\\ b_1$ $a_1\\ \\mathcal{R}\\ b_2$ $\\cdots$ $a_1\\ \\mathcal{R}\\ b_m$ $a_2$ $a_2\\ \\mathcal{R}\\ b_1$ $a_2\\ \\mathcal{R}\\ b_2$ $\\cdots$ $a_2\\ \\mathcal{R}\\ b_m$ $\\vdots$ $\\vdots$ $\\vdots$ $\\ddots$ $\\vdots$ $a_n$ $a_n\\ \\mathcal{R}\\ b_1$ $a_n\\ \\mathcal{R}\\ b_2$ $\\cdots$ $a_n\\ \\mathcal{R}\\ b_m$ Example 5.5.1 Consider the two sets\n\\[ \\begin{align*} A \u0026= \\{1, 2, 3, 4, 5\\} \\\\ B \u0026= \\{1, 3, 5, 7, 9\\} \\end{align*} \\]Define the relation $\\mathcal{R} \\subseteq A \\times B$ as\n$$a\\ \\mathcal{R}\\ b \\text{ if } a \\ge b$$Based on this definition, we bypass simply listing all the elements of $\\mathcal{R}$, and display them in a 0-1 table:\n$\\mathcal{R}$ $\\mathbf{A / B}$ 1 3 5 7 9 1 1 0 0 0 0 2 1 0 0 0 0 3 1 1 0 0 0 4 1 1 0 0 0 5 1 1 1 0 0 From this table, we see that the proposition $1 \\mathcal{R} 1 = 1$, meaning that the proposition $(1, 1) \\in \\mathcal{R} = 1$ as well. This was expected since we were given the relation’s definition as the “greater than or equal to” relation.\nThese kinds of tables are also useful when there is no clear “rule” determining whether or not certain elements between two sets are related. Of course the elements in a relation can be listed out, but a 0-1 table may make any patterns more evident (if any such patterns are present).\nExample 5.5.2 Consider the relation\n$$\\mathcal{R} = \\{(1, 2), (1, 3), (1, 4), (2, 1), (2, 3), (2, 4), (3, 1), (3, 2), (3, 4), (4, 1), (4, 2), (4, 3)\\}$$defined on the Cartesian Product $\\{1, 2, 3, 4\\}^2 = \\{1, 2, 3, 4\\} \\times \\{1, 2, 3, 4\\}$\nSome patient person out in the world may be able to work out what relation this is by inspecting that big list of ordered pairs, but for the rest of us, it helps to see a 0-1 table:\n$\\mathcal{R}$ $\\mathbf{\\{1, 2, 3, 4\\} / \\{1, 2, 3, 4\\}}$ 1 2 3 4 1 0 1 1 1 2 1 0 1 1 3 1 1 0 1 4 1 1 1 0 A quick look at this table reveals that the only entries that evaluate to false are the diagonal entries. All other entries evaluate to true. This suggests that a number is only related to another number if it is’nt equal. Hence, we have that\n$$a\\ \\mathcal{R}\\ b \\text{ if } a \\neq b$$This is the “not equal to” relation.\nExample 5.5.3 The math department at a certain college offers several courses: Calculus, Combinatorics, Linear Algebra, Statistics, Probability, Analysis, and Topology.\nThe department has surveyed its professors and have determined its “teaching preference” relation described by the following 0-1 table:\n$\\mathcal{R}$ $\\mathbf{P / C}$ Calculus Combinatorics Linear Algebra Statistics Probability Analysis Topology Smith 0 1 1 0 0 1 1 Gonzalez 0 0 1 1 0 0 1 Garcia 1 0 0 1 1 0 0 Ba 0 1 0 1 1 1 0 Khan 1 1 0 0 1 0 1 Of course, we see that relation $\\mathcal{R}$ is defined on the Cartesian Product $P \\times C$ where\n\\[ \\begin{align*} P \u0026= \\{\\text{Smith}, \\text{Gonzales}, \\text{Garcia}, \\text{Ba}, \\text{Khan} \\} \\\\ C \u0026= \\{\\text{Calculus}, \\text{Combinatorics}, \\text{Linear Algebra}, \\text{Statistics}, \\text{Probability}, \\text{Analysis}, \\text{Topology}\\} \\end{align*} \\]Based on the table above we see that\n\\[ \\begin{array}{ c c c c } \\text{Smith}\\ \\mathcal{R}\\ \\text{Combinatorics} \u0026 \\text{Smith}\\ \\mathcal{R}\\ \\text{Linear Algebra} \u0026 \\text{Smith}\\ \\mathcal{R}\\ \\text{Analysis} \u0026 \\text{Smith}\\ \\mathcal{R}\\ \\text{Topology} \\end{array} \\]meaning that Smith likes to teach Combinatorics, Linear Algebra, Analysis, and Topology.\nOn the other hand, we see that\n\\[ \\begin{array}{ c c c } \\text{Garcia}\\ \\mathcal{R}\\ \\text{Calculus} \u0026 \\text{Garcia}\\ \\mathcal{R}\\ \\text{Statistics} \u0026 \\text{Garcia}\\ \\mathcal{R}\\ \\text{Probability} \\end{array} \\]meaning that Garcia has a preference for teaching Calculus, Statistics, and Probability.\nArrow Diagrams Something else we could do to visualize a relation is by directly drawing arrows between related elements. The basic idea here is that for a relation $\\mathcal{R} \\subseteq A \\times B$, we list the elements of A and the elements of B with some space in between. (The lists could be vertical or horizontal.) Next, for each element $a_i$ in A, we draw an arrow from $a_i$ to an element $b_j$ in B if $a_i$ is related to $b_j$.\nExample 5.5.4 Eustice is the manager of a technical support team for a company that maintains various pieces of equipment to service customers. Everyday, Eustice assigns technicians to monitor a piece of equipment. Sometimes, more than one technician works on a piece of equipment. $$~$$ The team consists of Alex, Brady, Chelsey, Danielle, Ed, Frank, and Gertrude. Everyday, the team must monitor a server, a router, a firewall, and a database. On a particular day, Eustice made the following assignments for the equipment, resulting in the \"assignment\" relation. It should be apparent from Example 5.5.4 that arrow diagrams can look cluttered. Even with the visual clutter (which can be managed), arrow diagrams prove very useful, as we’ll see in the next chapter.\nTree Diagrams One shortcoming with the previous methods is that they do not easily accommodate Cartesian Products with three or more sets.\nTree diagrams easily accommodate more than two sets in a Cartesian Product. However, depending on how many elements are in the sets, tree diagrams can become quite unwieldy.\nA tree diagram is usually constructed starting from the left and progressing to the right. We start with a single dot, called the root of the tree. From this root, lines emanate to each of the elements in the first set listed in the Cartesian Product. Then, from each element in that first set, a line is extended to each of the elements from the second set listed in the Cartesian Product. However, each element of the first set gets its own copy of the elements in the second. Along each branch of the tree, we write the n-tuple containing all elements up to that branch in the tree. This process is repeated for each subsequent set in the Cartesian Product.\nExample 5.5.5 In a standard deck of playing cards, each card has a suit which can be one of Spades (♠), Hearts (♥), Diamonds (♦), or Clubs (♣). Furthermore, each card is colored either Red (R) or Black (B). Also consider a coin, which when flipped, either results in Heads (H) or Tails (T). We capture these outcomes in the following sets. \\[ \\begin{align*} \\text{Suit} \u0026= \\{\\text{♠}, \\text{♥}, \\text{♦}, \\text{♣}\\} \\\\ \\text{Color} \u0026= \\{\\text{B}, \\text{R}\\} \\\\ \\text{Flip} \u0026= \\{\\text{H}, \\text{T}\\} \\end{align*} \\] We construct the tree diagram for $\\text{Suit} \\times \\text{Color} \\times \\text{Flip}$ below: Notice in Example 5.5.5 that in the first level of the tree the elements are not enclosed in parentheses ( ). Since they are single elements, there is not much point in surrounding them, creating 1-tuples. At that point, we have not formed a Cartesian Product, so it’s unnecessary. Though in a sense, a single element could be considered a 1-tuple by itself. Either way, we intentionally do not enclose the first elements in ( ).\nRemember that order matters when writing the sets in a Cartesian Product. As such, we expect the tree diagram representing the Cartesian Product to look different based on how the sets are ordered.\nExample 5.5.6 Reconsider the sets from Example 5.5.5. A tree diagram for $\\text{Flip} \\times \\text{Suit} \\times \\text{Color}$ would look like this: To represent a relation, we simply remove any branches that yield n-tuples that are not in the relation. The act of removing such branches is sometimes called pruning in keeping with the arbor theme.\nExample 5.5.7 Reconsider the tree diagram from Example 5.5.5 drawn in a different style:\n( ♠ , B , ( H ♠ ) , B ( ) ♠ , B , T ) ♠ ( ♠ , R , ( H ♠ ) , R ( ) ♠ , R , T ) ( ♥ , B , ( H ♥ ) , B ( ) ♥ , B , T ) ♥ ( ♥ , R , ( H ♥ ) , R ( ) ♥ , R , T ) ( ♦ , B , ( H ♦ ) , B ( ) ♦ , B , T ) ♦ ( ♦ , R , ( H ♦ ) , R ( ) ♦ , R , T ) ( ♣ , B , ( H ♣ ) , B ( ) ♣ , B , T ) ♣ ( ♣ , R , ( H ♣ ) , R ( ) ♣ , R , T ) Notice that in the second level with the ordered pairs that ♠ is being paired up with the color red. We also see that ♥ is being paired up with the color black. However, in a normal deck of cards, those suits are not those colors.\nTo make an authentic deck of cards, lets consider the following relation:\n$$\\mathcal{R} = \\{(\\text{s}, \\text{c}, \\text{f})\\ |\\ \\bigl[(\\text{s} = ♠ \\lor ♣) \\land (\\text{c} = B)\\bigr] \\lor \\bigl[(\\text{s} = ♥ \\lor ♦) \\land (\\text{c} = R)\\bigr] \\}$$This relation says that if the card is in the relation and has a suit of spades or clubs, that would imply the card’s color is black. Similarly, if a card in the relation has a suit of either hearts or diamonds, that would imply that the card’s color is red.\nLet’s put a slash through each branch that yields ordered pairs not in $\\mathcal{R}$.\n( ♠ , B , ( H ♠ ) , B ( ) ♠ , B , T ) ♠ ( ♠ , R , ( H ♠ ) , R ( ) ♠ , R , T ) ( ♥ , B , ( H ♥ ) , B ( ) ♥ , B , T ) ♥ ( ♥ , R , ( H ♥ ) , R ( ) ♥ , R , T ) ( ♦ , B , ( H ♦ ) , B ( ) ♦ , B , T ) ♦ ( ♦ , R , ( H ♦ ) , R ( ) ♦ , R , T ) ( ♣ , B , ( H ♣ ) , B ( ) ♣ , B , T ) ♣ ( ♣ , R , ( H ♣ ) , R ( ) ♣ , R , T ) We don’t need to prune any sub-branches because they are considered pruned if their ancestor branches are pruned. We could go one step further and simply redraw the tree diagram without any of the pruned branches.\n( ♠ , B , ( H ♠ ) , B ( ) ♠ , B , T ) ♠ ♥ ( ♥ , R , ( H ♥ ) , R ( ) ♥ , R , T ) ♦ ( ♦ , R , ( H ♦ ) , R ( ) ♦ , R , T ) ( ♣ , B , ( H ♣ ) , B ( ) ♣ , B , T ) ♣ With less branches, we could make an equivalent tree diagram that is a little more compact:\n( ♠ , B , ( H ♠ ) ♠ , B ( ) ♠ , B , T ) ( ♥ , R , ( H ♥ ) ♥ , R ( ) ♥ , R , T ) ( ♦ , R , ( H ♦ ) ♦ , R ( ) ♦ , R , T ) ( ♣ , B , ( H ♣ ) ♣ , B ( ) ♣ , B , T ) Note that we don’t necessarily have to construct the tree for the entire Cartesian Product, and then prune branches to get the tree diagram for the desired relation. We could instead preemptively prune to prevent the tree from growing too large in the first place.\nExample 5.5.8 Reconsider the Cartesian Product $\\text{Flip} \\times \\text{Suit} \\times \\text{Color}$ from Example 5.5.6. Suppose we impose the following conditions:\nHeads can’t pair up with Hearts Tails can only pair up with Diamonds Of course, just like with Example 5.5.7, we would like to exclude any ordered triples that yield improperly colored cards. Instead of constructing the tree for the entire Cartesian Product, let’s just draw what we need:\n( H ( , H ♠ , , ♠ B ) ) ( H ( , H ♦ , H , ♦ R ) ) ( H ( , H ♣ , , ♣ B ) ) ( T ( , T ♦ , T , ♦ R ) )",
    "description": "As seen in the previous section, some relations involve nested structures. For example, some relations have elements that contain ordered pairs. Remember that all elements in a relation are ordered pairs themselves, meaning the elements of some relations are ordered pairs that also contain ordered pairs. As such, when we were listing out all of the elements of a relation, we sometimes spaced them out to make it easier to see what was in each of the elements. We saw this in Example 4.4.1, and Example 4.4.8.",
    "tags": [],
    "title": "Graphical Representations of Relations",
    "uri": "/the-book-of-foundational-mathematics/relations/graphical-representations-of-relations/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics",
    "content": "In the previous chapter, we studied a way to relate elements of one set to another set. However, we didn’t place any restrictions on those relations. Usually, we want to have some specific type of structure in the way we relate elements.\nThroughout our normal schooling, we encounter many types of these structured relations. Specifically, the kinds of relations we studied were functions. In this chapter, we delve deeper into these special kind of relations, and study them in an abstract sense. That is to say, what we discuss here will be applicable to all functions, not just those more familiar functions that relate real numbers to other real numbers.",
    "description": "In the previous chapter, we studied a way to relate elements of one set to another set. However, we didn’t place any restrictions on those relations. Usually, we want to have some specific type of structure in the way we relate elements.\nThroughout our normal schooling, we encounter many types of these structured relations. Specifically, the kinds of relations we studied were functions. In this chapter, we delve deeper into these special kind of relations, and study them in an abstract sense. That is to say, what we discuss here will be applicable to all functions, not just those more familiar functions that relate real numbers to other real numbers.",
    "tags": [],
    "title": "Functions",
    "uri": "/the-book-of-foundational-mathematics/functions/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Relations",
    "content": "In the previous couple of sections, we defined relations and described some ways to make it easier to visualize what elements were in those relations. Here, we take a closer look at relations defined on a single set. More specifically, we’ll look at some properties of a relation $\\mathcal{R}$ defined on a single set A (that is, $\\mathcal{R} \\subseteq A^2$).\nReflexive Relations Perhaps the easiest property to verify for each relation is whether or not every element of A is related to itself under the given relation.\nReflexive Let $\\mathcal{R} \\subseteq A^2$.\n$\\mathcal{R}$ is called reflexive on set $A$ if (and only if)\n$$\\forall a \\in A\\ [a\\ \\mathcal{R}\\ a]$$ Put another way, $\\mathcal{R}$ is reflexive when every single element in $A$ is related to itself.\nExample 5.6.1 a.) Consider the set $A = \\{1, 2, 3\\}$. Let $\\mathcal{R}_1 \\subseteq A^2$ be the relation\n$$\\mathcal{R}_1 = \\{(1, 1), (2, 2), (2, 3), (3, 3), (3, 1)\\}$$To determine if $\\mathcal{R}_1$ is reflexive, we have to see if every element in A is related to itself. The elements of $A$ are 1, 2, and 3. So, let’s see if the ordered pairs $(1, 1)$, $(2, 2)$, $(3, 3)$ are in the relation:\n$(1, 1) \\in \\mathcal{R}_1 = 1$ $(2, 2) \\in \\mathcal{R}_1 = 1$ $(3, 3) \\in \\mathcal{R}_1 = 1$ Since all of the above propositions are true, $\\mathcal{R}_1$ is indeed reflexive.\nb.) Reconsider the set $A = \\{1, 2, 3\\}$ with $\\mathcal{R}_2 \\subseteq A^2$ where\n$$\\mathcal{R}_2 = \\{(1, 2), (1, 1), (3, 3), (2, 3)\\}$$Even though $(1, 1) \\in \\mathcal{R}_2$ and $(3, 3) \\in \\mathcal{R}_2$, we have that $(2, 2) \\notin \\mathcal{R}_2$, meaning we have that $2\\ \\cancel{\\mathcal{R}_2}\\ 2$. As such, $\\mathcal{R}_2$ is not reflexive on A.\nc.) Reconsider $\\mathcal{R}_1 = \\{(1, 1), (2, 2), (2, 3), (3, 3), (3, 1)\\}$, but now let it be a relation defined on $B^2$ where $B = \\{1, 2, 3, 4\\}$.\nEven though $\\mathcal{R}_1$ was reflexive on $A$, we see that since $(4, 4) \\notin \\mathcal{R}_1$, we have that\n$$4\\ \\cancel{\\mathcal{R}_1}\\ 4$$meaning $\\mathcal{R}_1$ is not reflexive on $B$.\nNotice that based on Example 5.6.1, part c, that just because a relation $\\mathcal{R}$ is reflexive on one set, it is not necessarily reflexive on every set. Look again at the definition for reflexive. Notice how a relation is described as being reflexive on a set, and not just reflexive.\nLet’s look at a few more examples.\nExample 5.6.2 Consider the set $X = \\{1, 2, 3\\}$. Let $\\mathcal{R} \\subseteq \\mathcal{P}(X)^2$ where\n$$X_i\\ \\mathcal{R}\\ X_j\\ \\text{if}\\ X_i \\subseteq X_j$$In other words, $\\mathcal{R}$ is the subset relation on $\\mathcal{P}(X)$.\nAccording to Theorem 3.2.2, every set is a subset of itself. As such, we know that $\\mathcal{R}$ is reflexive on $\\mathcal{P}(X)$. However, since $|X| = 3$, it’s not difficult to visually verify this using a 0-1 table.\n$\\mathcal{R}$ $\\mathcal{P}(X)\\ /\\ \\mathcal{P}(X)$ $\\emptyset$ $\\{1\\}$ $\\{2\\}$ $\\{3\\}$ $\\{1, 2\\}$ $\\{1,3\\}$ $\\{2,3\\}$ $\\{1, 2, 3\\}$ $\\emptyset$ 1 1 1 1 1 1 1 1 $\\{1\\}$ 0 1 0 0 1 1 0 1 $\\{2\\}$ 0 0 1 0 1 0 1 1 $\\{3\\}$ 0 0 0 1 0 1 1 1 $\\{1, 2\\}$ 0 0 0 0 1 0 0 1 $\\{1, 3\\}$ 0 0 0 0 0 1 0 1 $\\{2, 3\\}$ 0 0 0 0 0 0 1 1 $\\{1, 2, 3\\}$ 0 0 0 0 0 0 0 1 Since the elements of $\\mathcal{P}(X)$ are listed in the same order along the rows and columns, we can essentially just check the truth values along the main diagonal from the top-left to the bottom-right. Since they are all true, that means all the elements are subsets of themselves (again, we already knew this from Theorem 3.2.2). Hence, $\\mathcal{R}$ is reflexive on $\\mathcal{P}(X)$.\nExample 5.6.3 Let $\\mathcal{R} \\subseteq \\mathbb{R}^2$ such that\n$$x\\ \\mathcal{R}\\ y\\ \\text{if}\\ x \\leq y$$meaning that $\\mathcal{R}$ is the “less-than-or-equal-to” relation.\nBecause there are infinitely many elements in $\\mathbb{R}$, we can’t simply check that for each $x \\in \\mathbb{R}$, we have that $x \\leq x$. Instead, we must use the definition of $\\leq$. Of course, every real number is either equal to itself, or less than itself (this is a disjunction of two propositions $x = x$ and $x \u003c x$):\n\\[ \\begin{align*} (x = x) \\lor (x \u003c x) \u0026= (1) \\lor (0) \\\\ \u0026= 1 \\end{align*} \\]Thus, $\\mathcal{R}$ is reflexive on $\\mathbb{R}$.\nNotice that in Example 5.6.3 we had to prove that each member of the set was related to itself using a definition. This will be necessary when the underlying set has infinitely many elements. On the other hand, in order to show that a relation is not reflexive on an infinite set, all we need is one counterexample, though finding the counterexample may be tricky at times.\nSymmetric Relations When examining the proposition $x\\ \\mathcal{R}\\ y$, it is natural to ask about the proposition $y\\ \\mathcal{R}\\ x$ as well.\nSymmetric Let $\\mathcal{R} \\subseteq A^2$.\n$\\mathcal{R}$ is called symmetric on set $A$ if (and only if)\n$$\\forall a,b \\in A\\ [a\\ \\mathcal{R}\\ b \\implies b\\ \\mathcal{R}\\ a]$$ Example 5.6.4 Consider the set $X = \\{1, 2, 3, 4\\}$\nLet $\\mathcal{R} \\subseteq X^2$ such that\n$$\\mathcal{R} = \\{(1, 1), (1, 4), (4, 1), (2, 2), (2, 3), (3, 2), (3, 3), (3, 4), (4, 3), (4, 4)\\}$$According to the definition of symmetric relations, we need\n$$\\forall a,b \\in X\\ [a\\ \\mathcal{R}\\ b \\implies b\\ \\mathcal{R}\\ a]$$which means we need that\n$$a\\ \\mathcal{R}\\ b \\rightarrow b\\ \\mathcal{R}\\ a = 1$$for every single ordered pair $(a, b)$ in $X^2$.\nBecause $(2, 3) \\in \\mathcal{R}$, we have that $2\\ \\mathcal{R}\\ 3 = 1$. But we also have that $(3, 2) \\in \\mathcal{R}$, meaning we also have that $3\\ \\mathcal{R}\\ 2 = 1$. Thus,\n\\[ \\begin{align*} 2\\ \\mathcal{R}\\ 3 \\rightarrow 3\\ \\mathcal{R}\\ 2 \u0026= 1 \\rightarrow 1 \\\\ \u0026= 1 \\end{align*} \\]Of course, we also see that since $(1, 1) \\in \\mathcal{R}$, we have that $1\\ \\mathcal{R}\\ 1 = 1$. Therefore we have that\n\\[ \\begin{align*} 1\\ \\mathcal{R}\\ 1 \\rightarrow 1\\ \\mathcal{R}\\ 1 \u0026= 1 \\rightarrow 1 \\\\ \u0026= 1 \\end{align*} \\]We need to check the implication $a\\ \\mathcal{R}\\ b \\rightarrow b\\ \\mathcal{R}\\ a$ for every single ordered pair $(a, b) \\in X^2$. If even one of the implications is false, then we’ll know that $\\mathcal{R}$ is not symmetric.\nWe organize our checking using a modified truth table. The first two columns will show the value of $a$ and $b$, but will still show truth values for the propositions $(a\\ \\mathcal{R}\\ b)$, $(b\\ \\mathcal{R}\\ a)$, and their implication.\n$\\mathbf{a}$ $\\mathbf{b}$ $a\\ \\mathcal{R}\\ b$ $b\\ \\mathcal{R}\\ a$ $(a\\ \\mathcal{R}\\ b) \\rightarrow (b\\ \\mathcal{R}\\ a)$ 1 1 1 1 1 1 2 0 0 1 1 3 0 0 1 1 4 1 1 1 2 1 0 0 1 2 2 1 1 1 2 3 1 1 1 2 4 0 0 1 3 1 0 0 1 3 2 1 1 1 3 3 1 1 1 3 4 1 1 1 4 1 1 1 1 4 2 0 0 1 4 3 1 1 1 4 4 1 1 1 All of the necessary implications are true, which means $\\mathcal{R}$ is symmetric on $X$.\nWe also see that $\\mathcal{R}$ is reflexive.\nExample 5.6.5 Consider the relation $\\mathcal{R} \\subseteq X^2$ on $X = \\{1, 2, 3, 4\\}$ where\n$\\mathcal{R} = \\{(1, 2), (2, 3), (3, 4)\\}$\nThis relation is not symmetric because $(2, 1) \\notin \\mathcal{R}$. This tells us that\n$$1 \\mathcal{R} 2 \\rightarrow 2 \\mathcal{R} 1 = 1 \\rightarrow 0 = 0$$ Just like with reflexive relations, we can represent these relations using 0-1 tables. And just like with reflexive relations, there is a pattern we can exploit to determine if the relation is symmetric.\nFor any ordered pair $(a, b) \\in \\mathcal{R}$, the element in the $a^{th}$ row and the $b^{th}$ column will be a 1. Thus, in order for $\\mathcal{R}$ to be symmetric, we’d also need $(b, a) \\in \\mathcal{R}$ as well, meaning that the element in the $b^{th}$ row and $a^{th}$ column also needs to be 1. This means that the 0-1 table needs to be symmetric around the main diagonal.\nExample 5.6.6 Consider the relation $\\mathcal{R} \\subseteq X^2$ on $X = \\{1, 2, 3, 4\\}$ where\n$$\\mathcal{R} = \\{(1, 3), (3, 1), (2, 4), (4, 2)\\}$$The associated 0-1 table for $\\mathcal{R}$ is\n$\\mathbf{\\mathcal{R}}$ $\\mathbf{X / X}$ 1 2 3 4 1 0 0 1 0 2 0 0 0 1 3 1 0 0 0 4 0 1 0 0 Since this table is symmetric about the main diagonal line, $\\mathcal{R}$ is symmetric.\nSince the main diagonal has 0s, $\\mathcal{R}$ is not reflexive.\nExample 5.6.7 Consider the relation $\\mathcal{R} \\subseteq X^2$ where $X = \\{1, 2, 3, 4\\}$ with 0-1 table\n$\\mathbf{\\mathcal{R}}$ $\\mathbf{X / X}$ 1 2 3 4 1 1 1 0 0 2 0 1 1 0 3 0 0 1 1 4 0 0 0 1 Notice that this table is not symmetric about the main diagonal since $(1, 2)$ has truth value 1, but $(2, 1)$ has truth value 0.\nSince the main diagonal is all 1s, $\\mathcal{R}$ is reflexive.\nLet’s look at one more example involving a relation defined on the real numbers.\nExample 5.6.8 Let $\\mathcal{R}_1 \\subseteq \\mathbb{R}^2$ such that\n$$x\\ \\mathcal{R}_1\\ y\\ \\text{if}\\ xy \u003e 0$$We know that this relation is symmetric because multiplication is commutative:\n\\[ \\begin{align*} x\\ \\mathcal{R}_1\\ y \u0026\\implies xy \u003e 0 \\\\ \u0026\\implies yx \u003e 0 \\\\ \u0026\\implies y\\ \\mathcal{R}_1\\ x \u0026\\end{align*} \\]Notice that $\\mathcal{R}_1$ is not reflexive because even though $0 \\in \\mathbb{R}$, the fact that $(0)(0) \\ngtr 0$ means that $0\\ \\cancel{\\mathcal{R}_1}\\ 0$.\nHowever, the relation $\\mathcal{R}_2 \\subseteq \\mathbb{R}^2$ where $x\\ \\mathcal{R}_2\\ y\\ \\text{if}\\ xy \\geq 0$ is reflexive.\nAntisymmetric Relations Suppose $(a, b) \\in \\mathcal{R}$ for some relation $\\mathcal{R} \\in A^2$. If the truth value in the $a^{th}$ row and $b^{th}$ column is the same truth value in the $b^{th}$ row and $a^{th}$ column, we say $\\mathcal{R}$ is symmetric on $A$. If this isn’t the case, then the 0-1 table would be not be symmetric about the main diagonal.\nHowever, we do not define antisymmetric as simply not symmetric.\nAntisymmetric Let $\\mathcal{R} \\subseteq A^2$.\n$\\mathcal{R}$ is called antisymmetric on set $A$ if (and only if)\n$$\\forall a, b \\in A\\ [a\\ \\mathcal{R}\\ b \\land b\\ \\mathcal{R}\\ a \\implies a = b]$$ Essentially, what this definition says is that the only way for two elements of $A$ to be related to each other is that they must be the same element. Remember that saying\n$$\\forall a, b \\in A\\ [a\\ \\mathcal{R}\\ b \\land b\\ \\mathcal{R}\\ a \\implies a = b]$$is the same thing as saying\n$$\\bigl(\\forall a, b \\in A\\ [a\\ \\mathcal{R}\\ b \\land b\\ \\mathcal{R}\\ a\\ \\rightarrow\\ a = b]\\bigr) = 1$$An example is in order.\nExample 5.6.9 Consider the relation $\\mathcal{R} \\subseteq \\{1, 2, 3\\}^2$ such that\n$$\\mathcal{R} = \\{(1, 1), (3, 3), (2, 1), (3, 2), (1, 3)\\}$$We see that since $(2, 1) \\in \\mathcal{R}$, we have that $2\\ \\mathcal{R}\\ 1$, but $(1, 2) \\notin \\mathcal{R}$, meaning $1\\ \\cancel{\\mathcal{R}}\\ 2$. As such, we see that\n\\[ \\begin{align*} (2\\ \\mathcal{R}\\ 1) \\land (1\\ \\mathcal{R}\\ 2) \\rightarrow (2 = 1) \u0026= 1 \\land 0 \\rightarrow 0 \\\\ \u0026= 0 \\rightarrow 0 \\\\ \u0026= 1 \\end{align*} \\]Above, we were treating $2 = 1$ as a (false) proposition.\nSince the above implication was true, it is still possible for $\\mathcal{R}$ to be antisymmetric. We have to check the implication\n$$(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ a) \\rightarrow (a = b)$$for every ordered pair $(a, b) \\in \\{1, 2, 3\\}^2$. If even one implication is false, then we’ll know that $\\mathcal{R}$ is not antisymmetric.\nJust like in Example 5.6.4, we’ll use a modified truth table to examine each of the propositions $(a\\ \\mathcal{R}\\ b)$, $(b\\ \\mathcal{R}\\ a)$, and $(a = b)$, including their conjunctions and implications.\n$\\mathbf{a}$ $\\mathbf{b}$ $a\\ \\mathcal{R}\\ b$ $b\\ \\mathcal{R}\\ a$ $(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ a)$ $a = b$ $(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ a) \\rightarrow (a = b)$ 1 1 1 1 1 1 1 1 2 0 1 0 0 1 1 3 1 0 0 0 1 2 1 1 0 0 0 1 2 2 0 0 0 1 1 2 3 0 1 0 0 1 3 1 0 1 0 0 1 3 2 1 0 0 0 1 3 3 1 1 1 1 1 Since all of the implications are true, we now know that $\\mathcal{R}$ is antisymmetric.\nWe noted above that we don’t define antisymmetric as not symmetric. This suggests that it’s possible for a relation to be simultaneously symmetric and antisymmetric. A relation could also be neither symmetric nor antisymmetric.\nExample 5.6.10 Let $\\mathcal{R}_1 \\subseteq \\{1, 2, 3\\}^2$ be the relation\n$$\\mathcal{R}_1 = \\{(1,1), (2,2)\\}$$First, let’s examine a modified truth table to determine if $\\mathcal{R}_1$ is symmetric on $X$:\n$\\mathbf{a}$ $\\mathbf{b}$ $a\\ \\mathcal{R}\\ b$ $b\\ \\mathcal{R}\\ a$ $(a\\ \\mathcal{R}\\ b) \\rightarrow (b\\ \\mathcal{R}\\ a)$ 1 1 1 1 1 1 2 0 0 1 1 3 0 0 1 2 1 0 0 1 2 2 1 1 1 2 3 0 0 1 3 1 0 0 1 3 2 0 0 1 3 3 0 0 1 The last column has all 1s, meaning $\\mathcal{R}_1$ is symmetric. Now we check a modified truth table to determine if $\\mathcal{R}_1$ is antisymmetric:\n$\\mathbf{a}$ $\\mathbf{b}$ $a\\ \\mathcal{R}\\ b$ $b\\ \\mathcal{R}\\ a$ $(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ a)$ $a = b$ $(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ a) \\rightarrow (a = b)$ 1 1 1 1 1 1 1 1 2 0 0 0 0 1 1 3 0 0 0 0 1 2 1 0 0 0 0 1 2 2 1 1 1 1 1 2 3 0 0 0 0 1 3 1 0 0 0 0 1 3 2 0 0 0 0 1 3 3 0 0 0 1 1 So $\\mathcal{R}_1$ is also antisymmetric. The only elements in $\\mathcal{R}_1$ are $(1, 1)$ and $(2, 2)$, which lie on the main diagonal in the 0-1 table. Every other value in the 0-1 table will be 0:\n$\\mathcal{R}_1$ $\\mathbf{\\{1, 2, 3\\} / \\{1, 2, 3\\}}$ 1 2 3 1 1 0 0 2 0 1 0 3 0 0 0 So, relations that are both symmetric and antisymmetric will only have truth values of 1 that lie along the main diagonal. If all entries along the diagonal were 1, then the relation would simultaneously reflexive, symmetric, and antisymmetric.\nExample 5.6.11 Let $\\mathcal{R}_2 \\subseteq \\{1, 2, 3\\}^2$ be the relation\n$$\\mathcal{R}_2 = \\{(1, 1), (2, 2), (1, 3), (3, 1), (2, 3)\\}$$Just like in Example 5.6.10, we start by checking if $\\mathcal{R}_2$ is symmetric:\n$\\mathbf{a}$ $\\mathbf{b}$ $a\\ \\mathcal{R}\\ b$ $b\\ \\mathcal{R}\\ a$ $(a\\ \\mathcal{R}\\ b) \\rightarrow (b\\ \\mathcal{R}\\ a)$ 1 1 1 1 1 1 2 0 0 1 1 3 1 1 1 2 1 0 0 1 2 2 1 1 1 2 3 1 0 0 3 1 1 1 1 3 2 0 1 1 3 3 0 0 1 There is a 0 in the last column, meaning $\\mathcal{R}_2$ is not symmetric. We know this because $(2, 3) \\in \\mathcal{R}_2$, but $(3, 2) \\notin \\mathcal{R}_2$. So the presence of an antisymmetric ordered pair $(2, 3)$ is preventing $\\mathcal{R}_2$ from being symmetric.\nNow we check if $\\mathcal{R}_2$ is antisymmetric:\n$\\mathbf{a}$ $\\mathbf{b}$ $a\\ \\mathcal{R}\\ b$ $b\\ \\mathcal{R}\\ a$ $(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ a)$ $a = b$ $(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ a) \\rightarrow (a = b)$ 1 1 1 1 1 1 1 1 2 0 0 0 0 1 1 3 1 1 1 0 0 2 1 0 0 0 0 1 2 2 1 1 1 1 1 2 3 1 0 0 0 1 3 1 1 1 1 0 0 3 2 0 1 0 0 1 3 3 0 0 0 1 1 The presence of two 0s in the last column means $\\mathcal{R}_2$ is not antisymmetric. The presence of the two symmetric ordered pairs $(1, 3)$ and $(3, 1)$ prevents $\\mathcal{R}_2$ from being antisymmetric.\nLooking at the 0-1 table for $\\mathcal{R}_2$ reveals it is neither completely symmetric about the main diagonal, nor is it completely non-symmetric about the main diagonal:\n$\\mathcal{R}_1$ $\\mathbf{\\{1, 2, 3\\} / \\{1, 2, 3\\}}$ 1 2 3 1 1 0 1 2 0 1 1 3 1 0 0 Transitive Relations The final way we’ll describe relations is whether or not they’re transitive. To understand this property, let’s think about whole numbers.\nEarly on in our educations, we learn that the whole numbers 1, 2, 3, 4, $\\ldots$ have an order associated with them. We recognize that 2 \u003c 10, 42 \u003c 88, 2344 \u003c 1229383, and so on. Let’s focus on the number 50. We know that 50 \u003c 51. We also know that there are plenty of numbers less than 50. If we pick any one of the whole numbers less than 50 at random, what can we conclude?\nInitially, all we know about our randomly picked number, which we’ll refer to as $n$ is that it’s less than 50. But we also know that 50 \u003c 51. Thus, we know that since $n$ \u003c 50, and 50 \u003c 51, we can conclude that $n$ \u003c 51 as well.\nThis is the transitive property of \u003c on the whole numbers. We state a more general definition for any relation now:\nTransitive Let $\\mathcal{R} \\subseteq A^2$.\n$\\mathcal{R}$ is called transitive on set A if (and only if)\n$$\\forall a, b, c \\in A\\ [(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ c) \\implies (a\\ \\mathcal{R}\\ c)]$$ In the definition above, we can sort of think of element $b$ as an element that “bridges the gap”, so to speak, from element $a$ to element $c$.\nExample 5.6.12 Consider the relation $\\mathcal{R} \\subseteq \\{1, 2, 3\\}^2$ where\n$$\\mathcal{R} = \\{(1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (1, 3)\\}$$Because $(1, 2) \\in \\mathcal{R}$ and $(2, 3) \\in \\mathcal{R}$, in order for $\\mathcal{R}$ to be transitive, it would have to include $(1, 3)$ as well, which it does.\nUsing the definition of transitive, we see that\n\\[ \\begin{align*} (1\\ \\mathcal{R}\\ 2) \\land (2\\ \\mathcal{R}\\ 3) \\rightarrow (1\\ \\mathcal{R}\\ 3) \u0026= 1 \\land 1 \\rightarrow 1 \\\\ \u0026= 1 \\rightarrow 1 \\\\ \u0026= 1 \\end{align*} \\]But of course, the definition of transitive says that the implication must be true for all triples $a, b, c$ taken from $\\{1, 2, 3\\}$. There will be quite a few implications to check, so we’ll organize everything into another modified truth table:\na b c $a\\ \\mathcal{R}\\ b$ $b\\ \\mathcal{R}\\ c$ $(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ c)$ $a\\ \\mathcal{R}\\ c$ $(a\\ \\mathcal{R}\\ b) \\land (b\\ \\mathcal{R}\\ c) \\rightarrow (a\\ \\mathcal{R}\\ c)$ 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 3 1 1 1 1 1 1 2 1 1 0 0 1 1 1 2 2 1 1 1 1 1 1 2 3 1 1 1 1 1 1 3 1 1 0 0 1 1 1 3 2 1 0 0 1 1 1 3 3 1 1 1 1 1 2 1 1 0 1 0 0 1 2 1 2 0 1 0 1 1 2 1 3 0 1 0 1 1 2 2 1 1 0 0 0 1 2 2 2 1 1 1 1 1 2 2 3 1 1 1 1 1 2 3 1 1 0 0 0 1 2 3 2 1 0 0 1 1 2 3 3 1 1 1 1 1 3 1 1 0 1 0 0 1 3 1 2 0 1 0 0 1 3 1 3 0 1 0 1 1 3 2 1 0 0 0 0 1 3 2 2 0 1 0 0 1 3 2 3 0 1 0 1 1 3 3 1 1 0 0 0 1 3 3 2 1 0 0 0 1 3 3 3 1 1 1 1 1 Since the last column is all 1s, we now know that $\\mathcal{R}$ is transitive on $\\{1, 2, 3\\}$.\nAs demonstrated in Example 5.6.12, even with a set as small as $\\{1, 2, 3\\}$, it can take a bit of work to verify that any relation defined on $\\{1, 2, 3\\}^2$ is transitive.\nSometimes, it may be easier to try and find a triple of elements $a, b, c$ that prove a relation is not transitive.\nExample 5.6.13 Consider the relation $\\mathcal{R} \\subseteq \\{1, 2, 3, 4\\}^2$ where\n$$\\mathcal{R} = \\{(1, 2), (2, 3), (3, 4), (2, 4), (4, 1), (3, 1), (1, 3) , (4, 2)\\}$$We see that since $(1, 2), (2, 3), (1, 3) \\in \\mathcal{R}$, transitivity is not violated. However, we may notice that $(3, 1), (1, 3) \\in \\mathcal{R}$, but $(3, 3) \\notin \\mathcal{R}$, meaning transitivity is violated.\nThis is enough to demonstrate that $\\mathcal{R}$ is not transitive on $\\{1, 2, 3, 4\\}$.\nWe didn’t use a 0-1 table to verify transitivity because there is no obvious pattern for transitivity. You essentially have to check corresponding entries to ensure transitivity is upheld or violated. Later in this book, we’ll discuss a way to quickly tell, but it involves operating on the table in a special way.",
    "description": "In the previous couple of sections, we defined relations and described some ways to make it easier to visualize what elements were in those relations. Here, we take a closer look at relations defined on a single set. More specifically, we’ll look at some properties of a relation $\\mathcal{R}$ defined on a single set A (that is, $\\mathcal{R} \\subseteq A^2$).\nReflexive Relations Perhaps the easiest property to verify for each relation is whether or not every element of A is related to itself under the given relation.",
    "tags": [],
    "title": "Characterizing Relations",
    "uri": "/the-book-of-foundational-mathematics/relations/characterizing-relations/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Foundational Mathematics \u003e Relations",
    "content": "Consider the two relations $\\mathcal{R}_1 \\subseteq X \\times Y$ and $\\mathcal{R}_2 \\subseteq Y \\times Z$. Suppose we knew that $x\\ \\mathcal{R}_1\\ y$ and that $y\\ \\mathcal{R}_2\\ z$. Just like with transitivity, we may ask whether or not $x$ could be related to $z$, with $y$ acting as an intermediary.\nWhat we could do is form a new relation $\\mathcal{R}_3 \\subseteq X \\times Z$ where $(x, z) \\in \\mathcal{R}_3$ that captures this relationship between $x$ and $z$. This is exactly what a composite relation does.\nComposite Relation Let $\\mathcal{R}_1 \\subseteq X \\times Y$ and $\\mathcal{R}_2 \\subseteq Y \\times Z$ be any two arbitrary relations where $X, Y, Z$ are arbitrary sets.\nThe relation $\\mathcal{R} \\subseteq X \\times Z$ is called the composite relation of $\\mathcal{R}_1$ and $\\mathcal{R}_2$, and is denoted\n$$\\mathcal{R} = \\mathcal{R}_1 \\circ \\mathcal{R}_2$$where\n\\[ \\begin{align*} \\mathcal{R} \u0026= \\mathcal{R}_1 \\circ \\mathcal{R}_2 \\\\ \u0026= \\{(x, z) \\in X \\times Z\\ |\\ \\bigl((x, y) \\in \\mathcal{R}_1\\bigr) \\land \\bigl((y, z) \\in \\mathcal{R}_2\\bigr)\\} \\end{align*} \\] Example 5.7.1 Consider the sets\n\\[ \\begin{align*} X \u0026= \\{1, 2, 3\\} \\\\ Y \u0026= \\{a, b, c\\} \\\\ Z \u0026= \\{\\Box, \\bigstar, \\clubsuit\\} \\end{align*} \\]along with the following relations:\n\\[ \\begin{align*} \\mathcal{R}_1 \u0026= \\{(1, a), (2, c), (3, b), (2, a)\\} \\\\ \\mathcal{R}_2 \u0026= \\{(a, \\clubsuit), (b, \\Box), (c, \\bigstar)\\} \\\\ \\end{align*} \\]where $\\mathcal{R}_1 \\subseteq X \\times Y$ and $\\mathcal{R}_2 \\subseteq Y \\times Z$. To form the composite relation $\\mathcal{R}_1 \\circ \\mathcal{R}_2$, start by looking at the first ordered pair $(1, a)$. We need to look for ordered pairs in $\\mathcal{R}_2$ where the first element is $a$. Since there is only one such ordered pair $(a, \\clubsuit)$, we know that $(1, \\clubsuit) \\in \\mathcal{R}_1 \\circ \\mathcal{R}_2$.\nNext, we see that $2\\ \\mathcal{R}_1\\ a$ and $2\\ \\mathcal{R}_1\\ c$. From $\\mathcal{R}_2$, we see that $a\\ \\mathcal{R}_2\\ \\clubsuit$ and $c\\ \\mathcal{R}_2\\ \\bigstar$. This tells us that $(2, \\clubsuit) \\in \\mathcal{R}_1 \\circ \\mathcal{R}_2$ and $(2, \\bigstar) \\in \\mathcal{R}_1 \\circ \\mathcal{R}_2$.\nFinally, we have that $3\\ \\mathcal{R}_1\\ b$ and $b\\ \\mathcal{R}_2\\ \\Box$, meaning $(3, \\Box) \\in \\mathcal{R}_1 \\circ \\mathcal{R}_2$.\nNow we know that\n$$\\mathcal{R}_1 \\circ \\mathcal{R}_2 = \\{(1, \\clubsuit), (2, \\clubsuit), (2, \\bigstar), (3, \\Box)\\}$$ The Associative Property of Compound Relations We previously examined associativity with the Cartesian product of more than two sets. Now we look at the differences between\n\\[ \\begin{align*} \\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3 \\\\ \\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr) \\\\ \\mathcal{R}_1 \\circ \\mathcal{R}_2 \\circ \\mathcal{R}_3 \\end{align*} \\]if any exist.\nExample 5.7.2 Suppose we have the following sets:\n\\[ \\begin{align*} A \u0026= \\{1, 2, 3, 4\\} \\\\ B \u0026= \\{a, b, c\\} \\\\ C \u0026= \\{x, y, z\\} \\\\ D \u0026= \\{\\spadesuit, \\clubsuit, \\heartsuit, \\diamondsuit\\} \\end{align*} \\]along with relations $\\mathcal{R}_1 \\subseteq A \\times B, \\mathcal{R}_2 \\subseteq B \\times C, \\mathcal{R}_3 \\subseteq C \\times D$ where\n\\[ \\begin{align*} \\mathcal{R}_1 \u0026= \\{(1, b), (1, c), (3, a), (4, b)\\} \\\\ \\mathcal{R}_2 \u0026= \\{(a, x), (b, x), (c, y), (c, z)\\} \\\\ \\mathcal{R}_3 \u0026= \\{(x, \\heartsuit), (z, \\diamondsuit)\\} \\end{align*} \\]First, we compute $\\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3$:\n\\[ \\begin{align*} \\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3 \u0026= \\{(1, x), (1, y), (1, z), (3, x), (4, x)\\} \\circ \\mathcal{R}_3 \\\\ \u0026= \\{(1, \\heartsuit), (q, \\diamondsuit), (3, \\heartsuit), (4, \\heartsuit)\\} \\end{align*} \\]Next we compute $\\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr)$:\n\\[ \\begin{align*} \\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr) \u0026= \\mathcal{R}_1 \\circ \\{(a, \\heartsuit), (b, \\heartsuit), (c, \\diamondsuit)\\} \\\\ \u0026= \\{(1, \\heartsuit), (q, \\diamondsuit), (3, \\heartsuit), (4, \\heartsuit)\\} \\end{align*} \\]We see that $\\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3$ and $\\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr)$ are the same set. As such, we expect that $\\mathcal{R}_1 \\circ \\mathcal{R}_2 \\circ \\mathcal{R}_3$ is the same set as well.\nFrom Example 5.7.2, we see that it didn’t matter in what order we computed the relation compositions. Was that just a coincidence?\nTheorem 5.7.1 Let $A, B, C, D$ be arbitrary sets, and let \\(\\mathcal{R}_1 \\subseteq A \\times B,\\) \\(\\mathcal{R}_2 \\subseteq B \\times C,\\) and \\(\\mathcal{R}_3 \\subseteq C \\times D\\).\nWe then have that\n$$ \\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3 = \\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr)$$ Proof 5.7.1 General Strategy: Here, we’ll use an element argument to show that $\\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3$ is a subset of $\\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr)$ and vice-versa. We’ll appeal to the definition of relations, which means the implications we’ll use will be bidirectional.\n\\[ \\begin{array}{ r l l } \u0026 \\mathbf{(a, d) \\in \\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3} \u0026 \\textbf{Reason} \\\\ \\Longleftrightarrow \u0026 \\exists c \\in C\\ [(a, c) \\in \\mathcal{R}_1 \\circ \\mathcal{R}_2 \\land (c, d) \\in \\mathcal{R}_3] \u0026 \\text{Definition of Relation Composition} \\\\ \\Longleftrightarrow \u0026 \\exists c \\in C\\ \\exists b \\in B\\ [(a, b) \\in \\mathcal{R}_1 \\land (b, c) \\in \\mathcal{R}_2 \\land (c, d) \\in \\mathcal{R}_3] \u0026 \\text{Definition of Relation Composition} \\\\ \\Longleftrightarrow \u0026 \\exists b \\in B\\ \\exists c \\in C\\ [(a, b) \\in \\mathcal{R}_1 \\land (b, c) \\in \\mathcal{R}_2 \\land (c, d) \\in \\mathcal{R}_3] \u0026 \\text{Repeated Quantifiers can be swapped} \\\\ \\Longleftrightarrow \u0026 \\exists b \\in B\\ [(a, b) \\in \\mathcal{R}_1 \\land (b, d) \\in \\mathcal{R}_2 \\circ \\mathcal{R}_3] \u0026 \\text{Definition of Relation Composition} \\\\ \\Longleftrightarrow \u0026 (a, d) \\in \\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr) \u0026 \\text{Definition of Relation Composition} \\end{array} \\]Thus, we have shown that\n$$(a, d) \\in \\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3 \\Longleftrightarrow (a, d) \\in \\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr)$$meaning we have that\n$$\\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3 = \\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr)$$as desired.\nNote that because $\\bigl(\\mathcal{R}_1 \\circ \\mathcal{R}_2\\bigr) \\circ \\mathcal{R}_3 = \\mathcal{R}_1 \\circ \\bigl(\\mathcal{R}_2 \\circ \\mathcal{R}_3\\bigr)$, the expression\n$$\\mathcal{R}_1 \\circ \\mathcal{R}_2 \\circ \\mathcal{R}_3$$is unambiguous.\nMultiple Compositions When we examined the Cartesian product, we noted how we can take the Cartesian product of a set with itself multiple times. We can do the same thing with relations.\nExample 5.7.3 Consider the set $W = \\{1, 2, 3, 4\\}$ along with the relation $\\mathcal{R} \\subseteq W^2$ where\n$$\\mathcal{R} = \\{(1, 2), (2, 4), (3, 1), (4, 3)\\}$$Let’s compute $\\mathcal{R} \\circ \\mathcal{R}$:\n\\[ \\begin{align*} \\mathcal{R} \\circ \\mathcal{R} \u0026= \\{(1, 2), (2, 4), (3, 1), (4, 3)\\} \\circ \\{(1, 2), (2, 4), (3, 1), (4, 3)\\} \\\\ \u0026= \\{(1, 4), (2, 3), (3, 2), (4, 1)\\} \\end{align*} \\]Now we compute $\\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R}$:\n\\[ \\begin{align*} \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \u0026= \\mathcal{R} \\circ \\bigl(\\mathcal{R} \\circ \\mathcal{R}\\bigr) \\\\ \u0026= \\{(1, 2), (2, 4), (3, 1), (4, 3)\\} \\circ \\{(1, 4), (2, 3), (3, 2), (4, 1)\\} \\\\ \u0026= \\{(1, 3), (2, 1), (3, 4), (4, 2)\\} \\end{align*} \\]Next, we compute $\\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R}$:\n\\[ \\begin{align*} \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \u0026= \\mathcal{R} \\circ \\bigl(\\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R}\\bigr) \\\\ \u0026= \\{(1, 2), (2, 4), (3, 1), (4, 3)\\} \\circ \\{(1, 3), (2, 1), (3, 4), (4, 2)\\} \\\\ \u0026= \\{(1, 1), (2, 2), (3, 3), (4, 4)\\} \\end{align*} \\]We got the identity relation back! Let’s see what happens when we take one more composition:\n\\[ \\begin{align*} \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \u0026= \\mathcal{R} \\circ \\bigl(\\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R}\\bigr) \\\\ \u0026= \\{(1, 2), (2, 4), (3, 1), (4, 3)\\} \\circ \\{(1, 1), (2, 2), (3, 3), (4, 4)\\} \\\\ \u0026= \\{(1, 2), (2, 4), (3, 1), (4, 3)\\} \\end{align*} \\]Doing this gives us the original relation back. Of course, at this point we expect to cycle through all of the previous results if we take more compositions.\nHaving to constantly repeat the composition operator can be cumbersome. Let’s make dealing with repeated composition easier by borrowing the exponential notation used with repeated multiplication and repeated Cartesian products.\nFirst, let’s use the convention that\n$$\\mathcal{R}^1 = \\mathcal{R}$$Now we use the following rule:\n$$\\mathcal{R}^n = \\mathcal{R} \\circ \\mathcal{R}^{n - 1}$$Doing this, we have the following:\n\\[ \\begin{align*} \\mathcal{R}^1 \u0026= \\mathcal{R} \\\\ \\mathcal{R}^2 \u0026= \\mathcal{R} \\circ \\mathcal{R}^1 = \\mathcal{R} \\circ \\mathcal{R} \\\\ \\mathcal{R}^3 \u0026= \\mathcal{R} \\circ \\mathcal{R}^2 = \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\\\ \\mathcal{R}^4 \u0026= \\mathcal{R} \\circ \\mathcal{R}^3 = \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\\\ \\mathcal{R}^5 \u0026= \\mathcal{R} \\circ \\mathcal{R}^4 = \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\circ \\mathcal{R} \\\\ \\vdots \\end{align*} \\]We don’t have to worry about parentheses because of Theorem 5.7.1, so we can compute these compositions any way we want:\n\\[ \\begin{align*} \\mathcal{R}^{m + n} \u0026= \\mathcal{R}^m \\circ \\mathcal{R}^n \\\\ \u0026= \\underbrace{\\mathcal{R} \\circ \\cdots \\circ \\mathcal{R}}_{\\text{m times}} \\circ \\underbrace{\\mathcal{R} \\circ \\cdots \\circ \\mathcal{R}}_{\\text{n times}} \\end{align*} \\] Example 5.7.4 Consider the set $M = \\{1, 2, 3, 4\\}$ along with the relation $\\mathcal{R} \\subseteq M^2$ where\n$$\\mathcal{R} = \\{(1, 2), (2, 3), (3, 3), (4, 1)\\}$$First, we compute $\\mathcal{R}^2$:\n\\[ \\begin{align*} \\mathcal{R}^2 \u0026= \\mathcal{R} \\circ \\mathcal{R} \\\\ \u0026= \\{(1, 2), (2, 3), (3, 3), (4, 1)\\} \\circ \\{(1, 2), (2, 3), (3, 3), (4, 1)\\} \\\\ \u0026= \\{(1, 3), (2, 3), (3, 3), (4, 2)\\} \\end{align*} \\]Next, we compute $\\mathcal{R}^3$:\n\\[ \\begin{align*} \\mathcal{R}^3 \u0026= \\mathcal{R} \\circ \\mathcal{R}^2 \\\\ \u0026= \\{(1, 2), (2, 3), (3, 3), (4, 1)\\} \\circ \\{(1, 3), (2, 3), (3, 3), (4, 2)\\} \\\\ \u0026= \\{(1, 3), (2, 3), (3, 3), (4, 3)\\} \\end{align*} \\]In $\\mathcal{R}^3$, we see that all numbers in $M$ are related to $3$, thus we expect that $\\mathcal{R}^4$ will also relate all numbers in $M$ to $3$ as well:\n\\[ \\begin{align*} \\mathcal{R}^4 \u0026= \\mathcal{R} \\circ \\mathcal{R}^3 \\\\ \u0026= \\{(1, 2), (2, 3), (3, 3), (4, 1)\\} \\circ \\{(1, 3), (2, 3), (3, 3), (4, 3)\\} \\\\ \u0026= \\{(1, 3), (2, 3), (3, 3), (4, 3)\\} \\\\ \u0026= \\mathcal{R}^3 \\end{align*} \\]At this point, we can confidently say that\n$$\\mathcal{R}^3 = \\mathcal{R}^4 = \\mathcal{R}^5 = \\mathcal{R}^6 = \\cdots$$",
    "description": "Consider the two relations $\\mathcal{R}_1 \\subseteq X \\times Y$ and $\\mathcal{R}_2 \\subseteq Y \\times Z$. Suppose we knew that $x\\ \\mathcal{R}_1\\ y$ and that $y\\ \\mathcal{R}_2\\ z$. Just like with transitivity, we may ask whether or not $x$ could be related to $z$, with $y$ acting as an intermediary.\nWhat we could do is form a new relation $\\mathcal{R}_3 \\subseteq X \\times Z$ where $(x, z) \\in \\mathcal{R}_3$ that captures this relationship between $x$ and $z$. This is exactly what a composite relation does.",
    "tags": [],
    "title": "Composite Relations",
    "uri": "/the-book-of-foundational-mathematics/relations/composite-relations/index.html"
  },
  {
    "breadcrumb": "",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Maximum Mathematics",
    "uri": "/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Probability \u003e Conditional Probability and Independence",
    "content": "Content coming eventually…",
    "description": "Content coming eventually…",
    "tags": [],
    "title": "Conditional Probability",
    "uri": "/the-book-of-probability/conditional-probability-and-independence/conditional-probability/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics \u003e The Book of Probability \u003e Foundations of Probability",
    "content": "Content coming soon!",
    "description": "Content coming soon!",
    "tags": [],
    "title": "Sample Spaces and Events",
    "uri": "/the-book-of-probability/foundations-of-probability/sample-spaces-and-events/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Maximum Mathematics",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
